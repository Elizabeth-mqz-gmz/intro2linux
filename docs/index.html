---
title: "Tutorial de introducción al biocómputo en sistemas GNU/Linux"
author: "Pablo Vinuesa, Centro de Ciencias Genómicas - UNAM; http://www.ccg.unan.mx/~vinuesa; @pvinmex"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 3
  pdf_document:
    latex_engine: lualatex
    toc: yes
    number_sections: yes
    toc_depth: 3
---

# Presentación
Este apunte fue creado por [Pablo Vinuesa, CCG-UNAM](http://www.ccg.unan.mx/~vinuesa/) ([twitter: \@pvinmex](https://twitter.com/pvinmex)) para diversos cursos y talleres, como [Taller 3 - An&aacute;lisis comparativo de genomas microbianos: Pangen&oacute;mica y filoinform&aacute;tica](http://congresos.nnb.unam.mx/TIB2019/t3-analisis-comparativo-de-genomas-microbianos-pangenomica-y-filoinformatica/) de los [Talleres Internacionales de Bioinform&aacute;tica - TIB2019](http://congresos.nnb.unam.mx/TIB2019), celebrados en el [Centro de Ciencias Genómicas](http://www.ccg.unam.mx) de la [Universidad Nacional Aut&oacute;noma de M&eacute;xico](http://www.ccg.unam.mx), del 29 de julio al 2 de agosto de 2019.

Las versión actual (v.`r Sys.Date()`) contiene extensiones y adaptaciones para el [Taller de Ciencias Genómicas: de Moléculas a Ecosistemas](http://www.ccg.unam.mx/noticias/academicas/taller-de-ciencias-genomicas-de-moleculas-a-ecosistemas/), que impartimos investigadores del [CCG-UNAM](http://www.ccg.unam.mx) para alumn@s de la [Facultad de Ciencias - UNAM](http://www.fciencias.unam.mx/), así como para el curso de introducción a Linux que imparto en la [Licenciatura en Ciencias Genómicas - LCG de la  UNAM](https://www.lcg.unam.mx/).

Si éste es tu primer contacto con Linux, te recomiendo que leas primero esta [presentación sobre introducción al biocómputo en sistemas Linux - PDF](https://github.com/vinuesa/intro2linux/blob/master/docs/intro_biocomputo_Linux.pdf).

## Licencia y términos de uso
Este material didáctico lo distribuyo p&uacute;blicamente a trav&eacute;s de este [repositorio GitHub intro2linux](https://github.com/vinuesa/intro2linux) bajo la [**Licencia No Comercial Creative Commons 4.0**](https://creativecommons.org/licenses/by-nc/4.0/) 

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons Licence" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0</a>

<a rel="license" href="https://www.gnu.org/licenses/gpl-3.0.html"><img alt="GNU General Public License v3" style="border-width:0" src="https://www.gnu.org/graphics/gplv3-127x51.png" /></a><br />
El código se distribuye bajo la licencia <a rel="license" href="https://www.gnu.org/licenses/gpl-3.0.html">GNU General Public License v3</a>

## El Proyecto de Software Libre GNU

Bienvenid@s al mundo de cómputo y software libre del [proyecto GNU](https://www.gnu.org/). Antes de empezar este tutorial, sugiero que te informes sobre lo que es el proyecto [GNU](https://www.gnu.org/), iniciado por [Richard Stallman (*rms*)](https://en.wikipedia.org/wiki/Richard_Stallman) en 1983 y desarrollado por una enorme comunidad de programadores, y su relación con Linux, para formar el sistema de cómputo libre [GNU/Linux](https://www.gnu.org/gnu/linux-and-gnu.html). 

Así lo explica *rms* en su artículo [Linux y el sistema GNU](https://www.gnu.org/gnu/linux-and-gnu.es.html), del que cito los primeros dos párrafos abajo: 

<em>
 Muchos usuarios de ordenadores ejecutan a diario, sin saberlo, una versión modificada del sistema GNU. Debido a un peculiar giro de los acontecimientos, a la versión de GNU ampliamente utilizada hoy en día se la llama a menudo «Linux», y muchos de quienes la usan no se dan cuenta de que básicamente se trata del sistema GNU, desarrollado por el proyecto GNU.

  Efectivamente existe un Linux, y estas personas lo usan, pero constituye solo una parte del sistema que utilizan. <b>Linux es el núcleo</b>: el programa del sistema que se encarga de asignar los recursos de la máquina a los demás programas que el usuario ejecuta. El núcleo es una parte esencial de un sistema operativo, pero inútil por sí mismo, sólo puede funcionar en el marco de un sistema operativo completo. <b>Linux se utiliza normalmente en combinación con el sistema operativo GNU: el sistema completo es básicamente GNU al que se le ha añadido Linux</b>, es decir, <b>GNU/Linux</b>. Todas las distribuciones denominadas «Linux» son en realidad distribuciones GNU/Linux.
</em>

-- <cite>Richard M. Stallman</cite>

- [The GNU project](https://www.gnu.org/gnu/gnu-history.html)
- [Free Software Education](https://www.gnu.org/education/education.html)
- [Free Software - Richard Stallman](https://audio-video.gnu.org/video/rms-education-es-high-sub.en.ogv) <== no dejes de ver este video
- [GNU software](https://www.gnu.org/software/software.html)
- [la relación entre GNU y Linux](https://www.gnu.org/gnu/linux-and-gnu.html)

### La controversia sobre el nombre: ¿GNU/Linux o sólo Linux?

Hay que aclarar que destacados miembros de la comunidad de desarrolladores de software libre esgrimen diversos argumentos por los que consideran que el sistema no se debe llamar  [GNU/Linux](https://en.wikipedia.org/wiki/GNU/Linux_naming_controversy), favoreciendo el nombre de *Linux*. 

Sirva de ejemplo uno de los comentario de [Linus Torvalds](https://es.wikipedia.org/wiki/Linus_Torvalds):

<em>
Umm, this discussion has gone on quite long enough, thank you very much. It doesn't really matter what people call Linux, as long as credit is given where credit is due (on both sides). Personally, I'll very much continue to call it "Linux", ...

The GNU people tried calling it GNU/Linux, and that's ok. It's certainly no worse a name than "Linux Pro" or "Red Hat Linux" or "Slackware Linux" ...
 
Lignux is just a punny name—I think Linux/GNU or GNU/Linux is a bit more "professional" ...
</em>

-- <cite>Linus Torvalds</cite>

Es importante notar que muchas de las [distribuciones de Linux](https://es.wikipedia.org/wiki/Distribuci%C3%B3n_Linux), incluyendo [Ubuntu](https://ubuntu.com/), no siguen estrictamente el principio de software libre de GNU. Ubuntu, por ejemplo, incluye el repositorio $restricted$, que contiene paquetes soportados por los desarrolladores de [Ubuntu](https://es.wikipedia.org/wiki/Ubuntu) debido a su importancia, pero que no está disponible bajo ningún tipo de licencia libre. Ejemplos notables son los controladores propietarios de algunas tarjetas gráficas, como los de ATI y NVIDIA. Ello ha permitido que Ubuntu sea una de las distribuciones de Linux con mayor compatibilidad de *hardware* y facilidad de instalación. Por ello recomiendo y uso esta *distro*.

### Nota de agradecimiento y postura personal
En lo personal, soy pragmático y agnóstico en lo que respecta a la controversia. Estoy muy agradecido a toda la comunidad por haber desarrollado, integrado y puesto a nuestra disposición esta maravilla de entorno de cómputo libre:  *kernel*, sistema operativo y software. Por ello gustosamente doy crédito al proyecto [GNU](https://www.gnu.org/) y al el núcleo o [*kernel*](https://es.wikipedia.org/wiki/N%C3%BAcleo_Linux), desarrollado originalmente por [Linus Torvalds](https://es.wikipedia.org/wiki/Linus_Torvalds) y conocido actualmente como [núcleo Linux](https://www.kernel.org/). Desde el año 2004 realizo todo mi trabajo en este entorno.


Como académico coincido plenamente con la visión de *rms* en que [los centros de educación pública deberían de usar el entorno  GNU/Linux](https://audio-video.gnu.org/video/rms-education-es-high-sub.en.ogv), idealmente desde la secundaria. En diversos lugares, notablemente en India, este mensaje de *rms* ha calado hondo. El estado de Kerala fue pionero: [desde 2006 toda la educación pública de Kerala se hace en en el entorno GNU/Linux](https://web.archive.org/web/20061208075556/http://www.financialexpress.com/fe_full_story.php?content_id=138464), habiendo eliminado Windows y software privativo de las escuelas estatales. Otros estados como Karnataka, Gujarat, Assam, West Bengal siguieron rápidamente el ejemplo, incorporando [software libre y código fuente abierto - OSS´](https://www.gnu.org/home.es.html#mission-statement) como una clave de sus sistemas educativos. Es una tendencia respaldada actualmente por el gobierno Indio, como pueden leer aquí: [Adoption of Free and Open Source Software in India](https://www.ripublication.com/ijaer18/ijaerv13n16_44.pdf). El resultado es contundente - la India es actualmente una potencia mundial en cómputo y matemáticas. 

Mi modesta pero activa manera de contribuir a la promoción del uso de [software libre y código fuente abierto](https://es.wikipedia.org/wiki/Software_libre_y_de_c%C3%B3digo_abierto) en México y Latinoamérica es [divulgando el uso del ambiente de cómputo GNU/Linux a alumnos universitarios mediante cursos en diversos países de habla hispana](https://www.ccg.unam.mx/~vinuesa/cursos.html), y promoviendo su implementación en la universidad, particularmente en la [LCG-UNAM](https://www.lcg.unam.mx/). Me referiré a este ambiente de cómputo con ambos nombres (GNU/Linux y Linux), para expresar mi neutralidad al respecto de la controversia arriba mencionada, aunque tiendo a preferir el uso de GNU/Linux para remarcar el crédito a todos los componentes de esta extraordinaria "simbiosis".

### GNU - Documentación y recursos <a rel="GNU.org" href="https://www.gnu.org/"><img alt="GNU" style="border-width:0" src="https://www.gnu.org/graphics/heckert_gnu.transp.small.png" /></a>
Todo software de calidad debe de estar bien documentado. Sin duda la [documentación del software GNU](https://www.gnu.org/manual/manual.html) es extraordinaria. No dejes de visitar al menos estos recursos:

- [GNU - awk manual](https://www.gnu.org/software/gawk/manual/gawk.html)
- [GNU - bash manual](https://www.gnu.org/software/bash/manual/bash.html)


## Referencias adicionales
Una vez que domines los comandos básicos que se presentarán en este documento, recomiendo revisar
tutoriales más detallados y completos como los siguientes:

- [The Linux Command Line - a complete introduction. William E. Shotts, Jr. No Starch Press](http://linuxcommand.org/lc3_learning_the_shell.php#contents)
- [Bash Reference Manual](https://tiswww.case.edu/php/chet/bash/bashref.html)
- [Advanced Bash Scripting Guide](http://tldp.org/LDP/abs/html/)
- [Bioinformatics Data Skills: Reproducible and Robust Research with Open Source Tools. Vince Buffalo. O'Reilly Media 2014](http://freecomputerbooks.com/Bioinformatics-Data-Skills.html)

******

#  Primer contacto con un sistema Linux - navegación del sistema

## Conexión a un servidor y exploración de sus características básicas
Estas prácticas están diseñadas para correr en un servidor remoto, pero puedes hacerlo también en una sesión local, es decir, en tu máquina. 
Sólo tienes que poner en un directorio los archivos con los que vamos a trabajar, los cuales puedes [descargar del directorio intro2linux/data del sitio GitHub](https://github.com/vinuesa/intro2linux/tree/master/docs/data)

### **ssh** establecer sesion remota encriptada (segura) via ssh al servidor con número dado de IP

```
ssh -l $USER IP
```

- Nota: para poder desplegar el ambiente gráfico en el servidor remoto debemos hacer el login con:

```
ssh -X $USER@IP
```

Al establecer una sesión remota vía $ssh$, la inicias en tu $HOME$


### **hostname** muestra el nombre del host (la máquina a la que estoy conectado) y la IP
```{bash}
hostname
hostname -i
```

### **uname** muestra el sistema operativo del host
```{bash}
uname
uname -a
```

### **top** o **htop** muestran los procesos en ejecución y los recursos que consumen
```
# sales con q o CTRL-c
htop
```

## Exploración del sistema de archivos

El <b>sistema de archivos de GNU/Linux</b> tiene una estructura jerárquica de directorios y subdirectorios que penden del <b>directorio raíz</b> $/$.
Esta estructura puede ser visualizada convenientemente con el comando $tree$. (Nota: $tree$ tal vez no esté instalado en tu sistema).

### Imprime la estructura de directorios de tu home u otro directorio
<pre>
tree -d $HOME/cursos/

/home/vinuesa/cursos/
└── intro2linux
    ├── bin
    ├── data
    ├── docs
    │ └── data
    ├── intro2linux
    │ ├── sesion1
    │ └── sesion1_linux
    ├── participantes_Taller_CG_FC
    ├── pics
    └── tutorials
        └── awk
</pre>

- Nota: según el directorio que indiques, la salida puede ser copiosa. Usar con criterio. Puedes limitar el número de nivels (subdirectorios)
  a mostrar con -L # (Level number)

```
tree -d -L 2 $HOME/cursos/

/home/vinuesa/cursos/
└── intro2linux
    ├── bin
    ├── data
    ├── docs
    ├── intro2linux
    ├── participantes_Taller_CG_FC
    ├── pics
    └── tutorials

```

<!--
Y este es el sistema de archivos que pende del <b>directorio raíz</b> $/$

```{bash}
tree -d -L 1 /
```
-->

### **pwd** imprime la **ruta absoluta** del directorio actual
```{bash}
# dónde me encuentro en el sistema?
pwd
```

### **ls** lista contenidos del directorio
```{bash}
# Qué contiene el directorio actual?
#  => Nota: muchos comandos los preced con '| head' para listar sólo las primeras (10) líneas 
#           de la salida del primer comando, con el fin de que ésta no sea demasiado larga. 
#     Veremos el uso de pipes '|' en detalle más adelante. 
#           Por ahora ignóralo y teclea simplemente ls
ls | head
```

```{bash}
# mostrar todos (-a all) los archivos, incluidos los ocultos, y sus propiedades
ls -al | head -20
```

#### Veamos el contenido del **directorio raíz /**

- corramos un $ls$ sin argumentos
```{bash}
ls / | head
```

- mucha más información obtenemos con el formato largo de ls: $ls -l$
```{bash}
ls -l / | head
```

#### Veamos las primeras 5 entradas y últimas 5 del directorio /bin
```{bash}
ls /usr/local/bin | head
```


```{bash}
# idem, pero con detalles de permisos etc
ls -l /usr/local/bin | head 
```

```{bash}
# idem, pero ordenando los archivos por fechas de modificacion (-t), listando los mas recientes al final (-r), es decir, en orden reverso
ls -ltr /usr/local/bin | tail
```

### Expansión de caracteres con <b>*</b> y <b>?</b>
```{bash}
# lista los archivos en /bin que empiezan por las letras b y c
ls /bin/b* | head -5
ls /bin/c* | head -5
```

```{bash}
# lista los archivos en /bin que empiezan por la letra c seguida de uno o dos caracteres más
ls /bin/c? | head -5
ls /bin/c?? | head -5
```


<!-- ### >>> AVANZADO -->
<!--  ahora con un bucle for, con la sintaxis: [for ALIAS in LIST; do YOUR_CODE; done] -->


<!-- for letter in b c d e f; do echo ">>> $letter"; ls /bin/${letter}*; echo '--------------'; done -->

<!-- # ahora para todas las letras -->

<!-- for letter in {a..z}; do echo ">>> $letter"; ls /bin/${letter}*; echo '--------------'; done -->

### Opciones más comunes e importantes del comando $ls$

Es fundamental que domines estas opciones básicas de $ls$

- **-a** list all (lista archivo ocultos tipo .directorio o .archivo)
- **-l** long format (muestra permisos y otros atributos como usuario, grupo, tamaño del archivo y fecha de modificación)
- **-t** time sort (ordena por tiempo)
- **-r** reverse sort
- **-R** list subdirectories recursively
- **-S** size sort (ordena por tamaño)
- **-h** human readable (indica Kb, Mb, Gb ...)

### Ejemplos de opciones del comando $ls$
Recuerda que podemos indicar múltiples opciones usando un solo guión y que el orden de las opciones no importa. 

Es decir <code>ls -la</code> == <code>ls -l -a</code> == <code>ls -a -l</code>

Prueba los siguientes comandos y aprende bien estas opciones básicas de $ls$

- <code>ls -a</code>
- <code>ls -lt</code>
- <code>ls -ltr</code>
- <code>ls -altrh</code>
- <code>ls -lS</code>
- <code>ls -lh</code>


## **Permisos** (modo de un archivo)
Los sistemas UNIX y GNU/Linux, al contrario que los basados en MS-DOS, están diseñados desde un inicio para operar en modo <b>multiusuario</b> y <b>multitarea</b> (multitasking). 
Por ello el sistema operativo debe asegurar la privacidad de los archivos y directorios de cada usuario del sistema. 

### Usuario, grupo y resto del mundo (User, Group, Others ...) y permisos $rwx$
En sistemas UNIX y GNU/Linux cada archivo y directorio tiene unos permisos determinados de <b>lectura=$r$</b> <b>escritura=$w$</b> y <b>ejecución=$x$</b>
para el usuario, grupo y resto del mundo, asignados en ese orden (UGO).

Un <b>archivo regular</b>, escrito por el usuario tiene los siguientes permisos por defecto, como muestra el comando $ls -l$

```
ls -l | grep odp
-rw-r--r-- 1 vinuesa vinuesa 2993606 sep 30 10:52 intro_biocomputo_Linux_LCG.odp
```

Veamos lo que quiere decir. Para ello necesitamos separar la cadena de caracteres en los siguientes componentes

``` 
   U   G   O       usuario grupo
1  2   3   4  5    6       7
-|rw-|r--|r-- 1 vinuesa vinuesa

```

donde:

```
1. la posición 1 (-) indica que se trata de un archivo regular. Un directorio se indica con "d" y una liga simbólica con "l"
2. El grupo 2,3 y 4 de caraceres indican el "modo" del archivo (permisos) para el usuario (U), grupo (G) y otros (resto del mundo O), 
   separados por "|" para facilitar su visualización.
   En este caso el usuario tiene permisos de lectura (r) y escritura (w) sobre el archivo que no es ejecutable (-)
   El grupo y el resto del mundo sólo pueden leer el archivo, pero no modificarlo.
   
```

Un <b>directorio</b> generado por el usuario con el comando estándar $mkdir$ tiene los siguientes permisos por defecto, como muestra el comando $ls -l$

```
ls -l | grep intro
drwxr-xr-x 4 vinuesa vinuesa    4096 sep 29 22:56 intro2linux
```

- Ejercicio: lista los permisos (modo) de este directorio para U|G|O


### Tabla de atributos de los permisos

La siguiente tabla resume los atributos que tienen los permisos $r$, $w$, $x$ sobre archivos regulares y directorios:

| Atributo    | Archivos                                      | Directorios                                  |
| ------------|:--------------------------------------------- |:-------------------------------------------- |
| r           | abrir y leer                                  | listar contenidos si tiene +x                |
| w           | editar pero no renombrar/borrar (atributo dir)| permite generar archivos en dir, si tiene +x |
| x           | permite ejecutra archivo (programa) si +r     | permite entrar al directorio                 |


### **chmod** - cambiar el modo (permisos) de un archivo o directorio

Hay dos maneras de hacerlo: 

1. Usando <b>notación simbólica</b> para U|G|O y todos (a)

| Símbolo     | Significado                                   |
| ------------|:--------------------------------------------- |
| u           | usuario, el dueño del archivo o directorio    |
| g           | dueño del gruop                               |
| o           | otros (resto del mundo)                       |
| a           | todos (all); combinación de u,g,o             |


- Ejemplos

$chmod\ notación\ archivo|dir$

| Notación    | Significado                                   |
| ------------|:--------------------------------------------- |
| u+x         | da permiso de ejecución a usuario             |
| u-x         | revoca permiso de ejecución a usuario         |
| o-r         | otros (resto del mundo) no puede leer         |
| +x          | equivale a a+x                                |
| o-rw        | quitar a otros permisos de rw                 |
| u+x,go=-rx  | asignar +x a U, revocar a O permisos de rx    |


$chmod\ a+rx\ script.sh$ hace el archivo script.sh leíble y ejecutable para todos


2. Usando <b>representación octal</b>

Los sistemas de numeración $octal$ (base 8) y su primo el $hexadecimal$ (base 16)
se usan frecuentemente para expresar números en computadoras.

Los humanos usamos el sistama $decimal$ ya que (la mayoría) tenemos 10 dedos.
Las computadoras en cambio "nacieron con un solo dedo", por lo que cuentan usando el sistema $binario$ (base 2)
usando sólo 1s y 0s. Por tanto en binario, contamos así: 0,1, 10,11, 100,101, 110,111 ...

En $octal$, contamos así: 0,1,2,3,4,5,6,7, 10,11,12,13,14,15,16,17, 20,21 ...

Usando una cadena de tres dígitos octales, podemos de manera muy conveniente definir el modo de un archivo para U|G|O
acorde a la siguiente tabla


| octal  | binario  | modo del archivo   |
| -------|:-------- |:------------------ |
| 0      | 000      | ---                |
| 1      | 001      | --x                |
| 2      | 010      | -w-                |
| 3      | 011      | -wx                |
| 4      | 100      | r--                |
| 5      | 101      | r-x                |
| 6      | 110      | rw-                |
| 7      | 111      | rwx                |

De modo que combinando los octales 

**READ    = 4**

**WRITE   = 2**

**EXECUTE = 1**

con las posiciones U|G|O, define los modos:

|  USER  |  GROUP | OTHERS | MODE |
|:------:|:------:|:------:|:----:|
| r w x  | r w x  | r w x  | UGO  |
| 4 2 0  | 0 0 0  | 0 0 0  | 600  | 
| 4 2 1  | 4 0 1  | 4 0 1  | 755  |    

- Ejemplos
  - $chmod\ 755\ script.sh$ otorga a "script.sh" modo de: -rwxr-xr-x
  - $chmod\ 700\ script.sh$ otorga a "script.sh" modo de: -rwx------
  - $chmod\ 644\ script.sh$ otorga a "script.sh" modo de: -rw-r--r--


### **file** nos indica las propiedades de un archivo

```{bash}
# mostrar las características de los archivos con file
file assembly_summary.txt.gz
file linux_commands.tab
file intro2linux
```


## Moviéndonos por el sistema de archivos: comando <b>cd</b>
Para movernos por el sistema de archivos de un sistema Linux podemos hacer uso de la <b>ruta absoluta</b> o la <b>ruta relativa</b>.

### La **ruta absoluta**

Define la ruta a cualquier directorio o archivo en el sistema partiendo del <b>directorio raíz</b> $/$, como indica el comando $pwd$

#### de nuevo, ¿dónde estoy?: imprime directorio actual con <b>pwd</b>
```{bash}
pwd 
```

#### entra a un directorio cualquiera usando la ruta absoluta
```
# indicamos la ruta absoluta o "path" al directorio al que queremos entrar
cd /home/vinuesa/cursos/intro2linux/data
```

### La **ruta relativa**

Se define en relación al directorio actual en el que estás. 

#### entra a un subdirectorio bajo en directorio actual usando la ruta relativa

```
cd directorio
```

#### sube un nivel de directorio usando RUTA RELATIVA: <b>cd ..</b>
```{bash}
cd ..
```

#### sube dos niveles en la jerarquía de directorios usando RUTA RELATIVA: <b>cd ../..</b>
```{bash}
cd ../..
```

- ¿dónde estoy?
```{bash}
pwd 
```

#### regresa al directorio en el que estabas justo antes con <b>cd -</b>
```{bash}
cd -
pwd
```

#### regresa a tu home desde cualquier punto del sistema de archivos con <b>cd</b>
```{bash}
cd $HOME

# que es equivalente a:
cd ~ 

# o también a 
cd
```

- cd cambiar directorios con rutas absolutas <b>(/ruta/completa/al/dir) y relativas ../../</b>
```{bash}
# a dónde nos lleva este comando?
cd /
pwd
```

- cambia de nuevo a tu home
```{bash}
cd -
pwd
```

- sube al directorio home/ usando la ruta relativa
```{bash}
cd ../
```

- y lista los contenidos
```{bash}
ls | head
```


## Generación de directorios: comando <b>mkdir</b>
```{bash}
# vamos a $HOME y generamos el directorio intro2linux
cd
if [ -d intro2linux ]; then 
    echo "found dir intro2linux"
else 
    mkdir intro2linux
fi
```

- comprueba los **permisos** del nuevo directorio
```{bash}
ls -ld intro2linux
```

- generemos un subdirectorio por debajo del que acabamos de crear:
```{bash}
mkdir -p intro2linux/sesion1 && cd intro2linux/sesion1
```

- cambiamos a /home/vinuesa e intenta crear estos mismos directorios ahí
```{bash}
cd /home/vinuesa && mkdir -p intro2linux/sesion1
```

No puedes escribir en mi directorio, porque no te he otorgado permiso para ello ;)


## Qué es exactamente un $comando$ en GNU/Linux?
Un comando de GNU/Linux puede ser una de cuatro cosas:

1. un **programa ejecutable**, como los que hemos estado usando, tales como $cut$, $sort$, $grep$ ... que están guardados en directorios del sistame tales como $/bin$, $/usr/bin$ ... Estos son *programas compilados*, escritos en lenguajes como $C$ o $C++$. Pero un comando también puede ser un *script* escrito en un lenguaje interpretado como $Bash$, $Perl$, $Phython$ o $R$, posiblemente escrito por el propio usuario. Estos frecuentemente se ubican en el directorio *$HOME/bin*

2. puede ser un **comando del shell**, un "$shell\ builtin$", como $cd$, $cp$ ...

3. una **función de shell**. Estos son miniprogramas de shell exportadas al ambiente (environment). Veremos más adelante cómo se delcaran funciones.

4. un **alias**, es decir un nombre de nuestra elección para otro comando. 

### Genera tu propio comando con un **alias**
El shell provee el comando integrado ("builtin") $alias$ que es muy útil para simplificar la llamada a comandos complejos que repetimos con frecuencia.

La sintaxis es la siguiente <code>alias NOMBRE="COMANDO"</code>

Es importante asegurarse que el nombre del alias no existe previamente en el sistema. Antes de generarlo, tecleo $tep$ en la consola para asegurarme que no existe un comando con dicho nombre.

```
vinuesa@alisio:~$ tep
-bash: tep: command not found
```

Ahora puedo generar el alias $tep$ para conectarme al servidor tepeu:

```
# Nota: uso asteriscos para no hacer público el NIS completo del server
alias tep="ssh -X vinuesa@tepeu.*.*.*"
```

Noten que el uso de comillas dobles encerrando el comando para el cual vamos a generar un *alias*

Una vez generado, puedo teclear $tep$, que me permite conectarme al servidor.

```
vinuesa@alisio:~$ tep
vinuesa@132.*.*.*'s password:
```

#### El archivo **$HOME/.bash_aliases**

Si quieres guardar una lista personal de aliases de manera permanente, puedes escribirlos en el archivo $\$HOME/.bash\_aliases$, *uno de los archivos de configuración del ambiente* que los usuarios avanzados de GNU/Linux guardan en su $\$HOME$

Cada vez que inicias tu máquina o que estableces una sesión remota, se lee este archivo en memoria, quedando los aliases como parte del *ambiente* ($environment$).

Van unas líneas de mi archivo $\$HOME/.bash\_aliases$ como ejemplos (los asteriscos los pongo para no hacer públicas las IPs)

```
# ssh aliases
alias puma="ssh -X vinuesa@132.*.*.*"            # desktop
alias ix="ssh -X vinuesa@132.*.*.*"              # ixchel ubuntu Linux server
alias tep="ssh -X vinuesa@132.*.*.*"             # tepeu Linux server

# fetch (sftp) data from servers
alias fyax="echo 'sftp vinuesa@*.*.unam.mx:'"
alias fku="echo 'sftp vinuesa@*.*.unam.mx:'"

# aliases for Java utils
alias figtree="java -Xms128m -Xmx512m -jar /home/vinuesa/soft_download/FigTree_v1.4.4/lib/figtree.jar"
alias seaview="/home/vinuesa/soft_download/seaview/seaview"

...
```

El archivo $\$HOME/.bash_aliases$ es leído por los archivos de configuración $\$HOME/.bashrc$ cuando iniciamos nuestra máquina local, o  $\$HOME/.bash\_profile$ cuando establecemos una sesión remota vía $ssh$. Hablaremos más de estos archivo en la sección de administración básica de un sistema Linux. 

Como ejercicio te sugiero que busques y leas estos archivos. Recuerda que tienes que usar $ls\ -a$ para poderlos ver, ya que el nombre de archivo está precedido por un **.** para ocultarlos a una llamada de $ls$ estándar.


### Identificación del tipo de comando con **type**
Podemos identificar a cuál de las cuatro categorías arriba listadas pertenece un comando particular. Veamos unos ejemplos:
```{bash}
type grep
type alias
```

### Muestra la ruta del comando con  **which**
Recuerda, podemos tener diferentes versiones de un mismo comando en diferentes directorios del sistema. 
El comando $which$ nos permite identificar la ruta absoluta del que primero ve el sistema en el $PATH$
```{bash}
which blastn
which perl
which trunc_seq.pl
which transpose_pangenome_matrix.sh
```

### Manual de cada comando:  <b>man command</b>
La mayoría de los programas ejecutables del sistema proveen un formato estandarizado de documentación 
que se despliega en la consola usando la línea de comandos
```{bash}
# mira las opciones de cut y sort en la manpage
man cut | head -30
```

### Ayuda de cada comando integrado en el Shell: <b>help shell_builtin</b> para *shell builtins*
```{bash}
# mira las opciones de cut y sort en la manpage
help cd
```


### Despliega información básica de uso del comando: <b>command --help</b>
Muchos programas del sistema soportan la opción $--help$ que despliega la descripción de la sintaxis del comando y las opciones. 
Esta suele ser una descripción más corta que la que nos dan las páginas de $man$ para el comando

```{bash}
# mira las opciones de cut y sort en la manpage
mkdir --help
```

### Encuentra comandos adecuados para un tipo de acción: <b>apropos</b>
Por ejemplo, quiero saber qué distribución de Linux corre mi máquina, pero no recuerdo el comando.
```{bash}
# encuentra comandos que tienen que ver con la palabra clave distribution
apropos distribution
```

- veo que es lsb_release. Consultemos su $--help$

```{bash}
# primero consulto su ayuda 
lsb_release --help
```

- ahora ejecuto el comando con la opción deseada

```{bash}
lsb_release -a
```

### Imprime desripciones de una línea de un comando con: <b>whatis</b>
Si no recuerdas o sabes lo que hace un comando particular, puedes obtener una respuesta rápida con $whatis\ command$
```{bash}
whatis cron
```


*****

# Trabajando con archivos: copiar, mover, renombrar y borrar archivos
```{bash}
# cambia a tu home, y luego a intro2linux/sesion1
cd && cd intro2linux/sesion1
```

## copia de archivo simple: <b> cp /path/to/file .</b>
- copia el archivo /home/vinuesa/intro2linux/data/linux_basic_commands.tab al directorio actual
```
cp /home/vinuesa/intro2linux/data/linux_basic_commands.tab . # <<< vean el punto, significa, dir actual
```

- otra manera, usando <b>rutas absolutas</b> y la <b>variable de ambiente $HOME</b>
```
cp /home/vinuesa/intro2linux/data/linux_basic_commands.tab $HOME/intro2linux/sesion1/
```

- otra variante, usando <b>rutas absolutas</b> y la tilde $~$ como abreviación de <b>$HOME</b>
```
cp /home/vinuesa/intro2linux/data/linux_basic_commands.tab ~/intro2linux/sesion1/
```

- copia un archivo a otro directorio renombrando la copia
```
cp ~/tmp/working_with_linux_commands.html ~/cursos/docs/index.html
```

## copiar un directorio: <b> cp -r dir . </b> [-r recursively, el directorio y su contenido]
- copiar el directorio /home/vinuesa/intro2linux/data/ a tu dir actual

```
# Noten el punto '.' y cp -r (recursively), necesario para copiar directorios completos
cp -r /home/vinuesa/intro2linux/data/ . 
```

- NOTA: podemos hacer uso de las combinaciones de ruta absoluta y/o relativas que más nos convengan.



## Eliminar un directorio: <b>rm -r[f]</b> [recursively -r and force -f]
```{bash}
mkdir borrame

cp linux_basic_commands.tab borrame

ls borrame

rm -rf borrame
```

Prueba ahora este comando
```
rm data
```
qué pasa?

¿Cómo tengo que borrar un directorio? <code>rm -r[f] directorio</code>
```
rm -rf data
```
- NOTA: en algunos sistemas la llamada a $rm\ -r\ directorio$ no permite borrar $directorio$ directamente, imprimiendo un mensaje de error. 
Par forzar el borrado, si el mensaje de advertencia, usamos $rm\ -rf\ directorio$

## <b>scp</b> copia de archivos entre máquinas vía Internet

Para llevar/copiar un archivo de tu máquina local al servidor, usamos el comando $scp$

Sintaxis: <code>scp ARCHIVO_LOCAL USUARIO@ip.maquina:/home/USUARIO/ruta/al/dir/destino/</code>

- **Ejercicio** con $scp$
  1. descarga el archivo [linux_basic_commands.tab](https://github.com/vinuesa/intro2linux/tree/master/docs/data/linux_basic_commands.tab) del repositorio GitHub a tu máquina
  2. cópialo a tu $\$HOME$ en el servidor


```
# asegúrate de estar en el directorio que contiene el archvo descargado:
ls linux_basic_commands.tab

scp linux_basic_commands.tab USUARIO@ip.maquina:/home/USUARIO/ruta/al/dir/destino/
```

- **NOTAS sobre scp**: 
 1. Si queremos copiar un directorio podemos usar la sintaxis <code>scp -r dir USUARIO@ip.maquina:/home/USUARIO/ruta/al/dir/destino/</code>
 2. Usa <code>scp -r dir destino</code> sólo si $dir$ contiene pocos archivos pequeños.
 3. Si necesitas copiar archivos grandes o directorios con muchos archivos, comprímelos primero con $gzip$ y $tar$ (ver más adelante)

## <b>sftp</b> descarga de archivos de una máquina remota a tu máquina local (laptop o desktop)

Para traer/copiar un archivo del servidor al directorio actual en la máquina local, usamos el comando $sftp$

Sintaxis: <code>sftp USUARIO@ip.maquina:/home/USUARIO/ruta/al/dir/destino/</code>

```
# asegúrate de estar en el directorio de tu máquina local donde quieres depositar el archivo a descargar del servidor:

cd $HOME/ruta/dir/destino

# ahora establecemos una sesión de ftp segura (sftp) a la máquina remoata
sftp USUARIO@ip.maquina:/home/USUARIO/ruta/al/dir/destino/

# podemos hacer un ls para buscar el/los archivo(s) que queremos descargar
ls *tab

# con la orden get, recuperamos los archivos deseados
get linux_basic_commands.tab
get *tab
get -r directorio

# cerrar la sesión sftp al serivdor
quit

# ésto nos regresa a $HOME/ruta/dir/destino
ls 
```

- **Ejercicio** con $sftp$
  1. genera el directorio *tmp_sftp* en tu máquina y métete en él
  2. copia de tepeu el archivo de tu elección a *tmp_sftp* meditante $sftp$


## Generación de ligas simbólicas a archivos: comando <b>ln -s  /ruta/al/archivo/fuente nombre_liga</b>
Hacer ligas simbólicas en vez de copiar cada archivo es muy importante, ya que permite ahorrar mucho espacio en disco al evitar la 
multiplicación de copias fisicas en el disco duro del mismo archivo en el $HOME de uno o más usuarios.

Sintaxis básica: 

- <code>ln -s /path/to/file .</code> # que genera una liga simbólica a /path/to/file, llamada file, en el directorio de trabajo
- <code>ln -s /path/to/file nombre_nuevo</code> # que genera una liga simbólica a /path/to/file, llamada nombre_nuevo, en el directorio de trabajo
- <code>ln -s /path/to/dir_with_many_files/*fastq.gz .</code> # que genera una liga simbólica en el directorio actual para cada archivo fastq.gz ubicado en /path/to/dir_with_many_files


```{bash}
hostn=$(hostname)
if [ "$hostn" == "alisio" ]; then
  ln -s /home/vinuesa/Cursos/TIB/TIB19-T3/sesion1_intro2linux/linux_basic_commands.tab comandos_de_linux.tab
elif [ "$hostn" == "puma" ]; then
  ln -s /home/vinuesa/Cursos/TIB19-T3/sesion1_intro2linux/linux_basic_commands.tab comandos_de_linux.tab
elif [ "$hostn" == "tepeu" ]; then 
   ln -s /home/vinuesa/intro2linux/data/linux_basic_commands.tab comandos_de_linux.tab 
   ln -s /home/vinuesa/intro2linux/data/assembly_summary.txt.gz .
fi

# confirmamos que se generaron las ligas
ls -l | head
```

- renombramos la liga (o cualquier archivo o directorio)
```{bash}
  mv comandos_de_linux.tab linux_commands.tab
```

- podemos elegir el nombre el apuntador que nos convenga
```
  ln -s nombre_largo_de_archivo.tgz arch_tmp.tgz
```

- podemos generar ligas simbólicas a todos los archivos de un directorio (por ej. lecturas fastq comprimidas de un secuenciador illumina) en el directorio de trabajo (actual) con un comando como éste:
```
  ln -s /export/data3/illumina_reads/run_stenos_2020-10-14/*fastq.gz .
```

## Nombres de archivos y directorios y uso de comillas sencillas y dobles
Los usuarios de Windows suelen escribir nombres de archivo con espacios: 'mi archivo de secuencias.fasta'. Como subrayamos en el [tutoral de introducción a GNU/Linux](https://github.com/vinuesa/intro2linux/tree/master/docs/intro_biocomputo_Linux.pdf), en el $Shell$ los espacios separan argumentos. Por ello lo mejor es usar guiones bajos para separar las partes del nombre de un archivo: 'recA_Bradyrhizobium.fna'. Recuerda también que en el $Shell$, la puntuación se considera, es decir 'recA_Bradyrhizobium.fna' y 'recA_bradyrhizobium.fna' son dos archivos distintos.

En las versiones modernas de GNU/Linux, los nombres de archivos con espacios son automáticamente mostrados entre comillas sencillas: 'mi archivo de secuencias.fasta'. Así el $Shell$ considera el nombre del archivo como un solo argumento.

### Uso de <b>comillas sencillas($'$)</b>: interpretación literal y escape de caracteres reservados para el <i>SHELL</i>

- Para renombrar o copiar un archivo que contiene espacies, debes escaparlo con comillar sencillas ($'$)

En el siguiente ejemplo las comillas sencillas sireven para interpretar literalmente los espacios como caracteres, 
por lo que el nombre de archivo es visto por el comando $mv$ como un solo argumento

```
mv 'mi archivo de secuencias.fasta' recA_Bradyrhizobium.fna
```

En ciertas ocasiones usarás <b>comillas sencillas ($'$)</b> para la ejecución de algunos programas desde la línea de comandos como por ejemplo al pasar opciones complejas a sed, perl, awk ... Estos son lenguajes interpretados, cuyo uso básico veremos más adelante


-  Ejemplo de llamada del comando $rename$ para eliminar espacios en nombres de archivos y directorios
```
# rename es un script de Perl que permite renombrar archivos de manera muy eficiente 
#   haciendo uso de expresiones regulares y expansión de nombres de archivo
# El siguiente ejemplo renombra todos los archivos docx del directorio, 
#   cabmiando todas las instancias (globalmente) de espacios por guiones bajos
rename 's/ /_/g' *.docx
```

- Ejemplo de llamada a $sed$ 

```
# con esta sentencia el editor de flujo sed cambiará todas las instancias de Windos por GNU-Linux
#  que aparecen en el archivo txt
sed 's/Windows/GNU-Linux/g' archivo.txt # cambia Windows por GNU-Linux globalmente ;)
```

### Uso de <b>comillas dobles ($"$)</b> para interpolación de variables

Se requieren <b>comillas dobles ($"$)</b> siempre que queramos interpolar una variable (obtener su valor) contenida dentro de una cadena de texto, 
como muestra el siguiente ejemplo

```{bash}
# echo imprime el contenido de la cadena acotada entre comillas dobles, interpolando las variables que tenga
echo "soy el usuario $USER en la máquina $HOSTNAME"
```

Si usáramos comillas sencillas, las variables no se interpolan y se imprime literalmente el nombre de las mismas

```{bash}
echo 'soy el usuario $USER en la máquina $HOSTNAME'
```


******

# Trabajando con archivos: visualización, generación y edición de archivos de texto plano (codificación ASCII)
Linux ofrece una amplia gama de programas para visualizar y editar archivos, todos con opciones poderosas y muy útiles. La elección de uno de ellos depende de lo que necesitamos o queremos. 

## Visualización de contenidos de archivos: comandos <b>head</b>, <b>tail</b>, <b>cat</b>, <b>less</b>, <b>more</b>
Los comandos $head$, $tail$, $cat$, $less$ y $more$ son algunos de los disponibles para visualizar el contenido de un archivo ASCII.

### Usa <b>head</b> y <b>tail</b> para desplegar la cabecera y cola de archivos
```{bash}
head linux_commands.tab 
```

```{bash}
tail linux_commands.tab
```

- Por defecto, $head$ y $tail$ despliegan 10 líneas. Pero podemos pasarle como opción el número de líneas deseado con esta sintaxis:

<code>tail -n -íntegro archivo</code> o simplemente con: <code>tail -íntegro archivo</code>

como se muestra en esto ejemplos:

```{bash}
# le podemos indicar el numero de lineas a desplegar
head -3 linux_commands.tab
```

```{bash}
tail -1 linux_commands.tab

```

- $tail$ tiene otras dos opciones que uso frecuentemente:
  - <code>tail -n +íntegro archivo</code>  # para imprimir a partir de la fila número **-n +íntegro** (permite eliminar primeras líneas)
  - <code>tail -f archivo</code> # opción **follow**, que permite ver cómo crece la  cola de un archivo a medida que se va escribiendo.

Ejemplo de <code>tail -n +íntegro archivo</code>

```{bash}
# imprime a partir de la línea 2, es decir, elimina cabecera del archivo
tail -n +2 linux_commands.tab | head -2
```


### <b>cat</b> despliega uno o más archivos, concatenándolos si son varios
```{bash}
cat linux_commands.tab | head
```

<b>cat -n</b> nos permite añadir números de línea a los archivos desplegados
```{bash}
cat -n linux_commands.tab | head
```

#### **cat** también permite generar contenidos copiados con el ratón o tecleando

A veces queremos copiar o teclear algún texto corto a un archivo. Esto podemos hacerlo abriendo un editor de textos como $vim$ o $gedit$, pero lo más rápido es hacerlo con $cat$ usando esta sintaxis:

```
cat > archivo.txt
AQUI PEGO O TECLEO EL TEXTO
Va otra línea
y otra

CTRL-d
```

- **Ejercicio**: 
  1. trata de generar el archivo borrame.txt haciendo uso del comando $cat$ como se indica arriba y que contenga algún texto de tu elección.
  2. despliega el contenido de dicho archivo con $cat$
  3. despliega el archivo sin su primera línea usando $tail\ -n\ +2$
  4. borra el archivo

### el paginador <b>less</b> despliega archivos página a página

El paginador $less$ es el más poderoso (otra alternativa ess $more$, pero extrañamente: $less$ is more!)

- sintaxis: <code>less archivo</code>
- opciones más comunes
  - con la barra espaciadora o avance de página puedes ir pasando páginas
  - con inicio o fin vas a dichas páginas
  - **/** te permite buscar un texto: prueba $/grep$
  - **-L** nos permitiría navegar horizontalmente archivos con líneas largas
  - con $q$ sales (quit)

Ejemplo 1:
```
less linux_commands.tab
/grep
q
```

Nota: con $q$ salimos del paginador $less$

Ejemplo 2: 
```
# less -L archivo nos permitiría navegar horizontalmente archivos con líneas largas, como tablas grandes
less -L linux_commands.tab 
```

$less$ tiene muchas más opciones: explóralas con <code>less --help</code>

### <b>zcat</b> y <b>zless</b> permiten visualizar el contenido de archivos de texto con compresión gzip (gnuzip)
Los archivos usados en genómica son frecuentemente grandes y/o numerosos, por lo que es una muy buena práctica
mantenerlos comprimidos para ocupar el mínimo espacio posible en disco. los comandos $zcat$ y $zless$ permiten
visualizar su contenido sin tenerlos que descomprimir. Veremos más comandos, como $zgrep$ que pueden trabajar
sobre archivos de texto comprimidos.

- $zcat$ desplegará todo el archivo, pudiéndolo pasar a otras herramientas de filtrado
```{bash}
zcat assembly_summary.txt.gz | head -5 | cut -f1-5
```

- usa $zless$ para navegar el archivo, página a página o usando las múltiples opciones de $less|zless$
```
zless assembly_summary.txt.gz | head -5 | cut -f1-5
```

## Edición de archivos con los editores <b>vim</b> o <b>[g|n]edit</b>
<b>vim</b> (vi improved) es un **poderoso editor programable** presente en todos los sistemas GNU/Linux. La principal característica tanto de Vim como de Vi consiste en **que disponen de diferentes modos** (*normal*, *visual*, *insert*, *command-line*, *select*, and *ex*) entre los que se alterna para realizar ciertas operaciones, lo que los diferencia de la mayoría de editores comunes, que tienen un solo modo (*insert*), en el que se introducen las órdenes mediante combinaciones de teclas (o interfaces gráficas). 

Vim Se controla por completo mediante el teclado desde un terminal, por lo que puede usarse sin problemas a través de conexiones remotas ya que no carga el sistema al no desplegar un entorno gráfico. 

Es muy recomendable aprender a usar Vim, pero no tenemos tiempo de hacerlo en el Taller, por lo que les recomiendo este [tutorial de uso de Vim en español](https://openwebinars.net/blog/vim-manual-de-uso-basico/), o directamente en su terminal tecleando el comando 

```
 vimtutor
 
 # para salir de vim, 
 
 <ESC> # para estar seguros que estamos en modo ex 
 :q
```

En el taller usaremos generalmente el editor con ambiente gráfico <b>gedit</b>, de uso muy sencillo.

```
# noten el uso de & al final de la sentencia para enviar el proceso al fondo
# para evitar que bloquee la terminal
gedit linux_commands.tab &

```

## Edición de archivos con el editor de flujo <b>sed</b> (stream editor)
$sed$ (stream editor) es un **editor de flujo**, una potente herramienta de tratamiento de texto para el sistema operativo UNIX que acepta como entrada un archivo, lo lee y modifica línea a línea de acuerdo a un script, mostrando el resultado por salida estándar (normalmente en pantalla, a menos que se realice una redirección). Sed permite manipular flujos de datos, como por ejemplo cortar líneas, buscar y reemplazar texto (con soporte de <b>[expresiones regulares](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular)</b> ), entre otras cosas. Posee muchas características de $ed$ y $ex$. 

Pueden consultar esta página para aprender lo básico de <b>[expresiones regulares](https://www.rexegg.com/regex-quickstart.html)</b>

La sintaxis general de la orden $sed$ es:

```
$ sed [-n] [-e'script'] [-f archivo] archivo1 archivo2 ...
```
donde:

    -n indica que se suprima la salida estándar.
    -e indica que se ejecute el script que viene a continuación. Si no se emplea la opción -f se puede omitir -e.
    -f indica que las órdenes se tomarán de un archivo

### Ejemplos de uso básico de <b>sed</b>: **sustituciones s///**

- Sustituciones $s///$ de palabras $s/esto/aquello/$ o caracteres en archivos de texto
```
$ sed 's/Windows/Linux/g' archivo
```

Noten en el ejemplo anterior el uso del modificador $///g$, que indica hacer las sustituciones de manera **global**, 
es decir en cada instancia de la ocurrencia de "/ésto//" en la línea. Si no se usa **g**, sólo se sustituye la primera instancia.

Ejemplos de uso básico de $sed$

- imprime los directorios de $\$PATH$, uno por línea
```{bash}
echo "# >>> sin modificador global <<<"
echo $PATH | sed 's/:/\n/'  
echo && echo "# >>> con modificador global <<<"
echo $PATH | sed 's/:/\n/g' # con modificador global
```

- cambia espacios sencillos por guiones bajos
```{bash}
head -1 linux_commands.tab                 # usamos head -1 para ver la primera línea, que modificaremos con sed
head -1 linux_commands.tab | sed 's/ /_/'  # se sustituye sólo la primera instancia de espacio en blanco!
head -1 linux_commands.tab | sed 's/ /_/g' # ahora globalmente
```

### Ejemplos de uso básico de <b>sed</b>: **cambio de fuente: y///**
- Cambia todas las minúsculas a mayúsculas de archivo:
```{bash}
head -1 linux_commands.tab | sed 'y/abcdefghijklmnopqrstuvwxyz/ABCDEFGHIJKLMNOPQRSTUVWXYZ/'
```

- Podemos combinar diversas acciones de sustitución, separándolas así: **s///; s///**
```{bash}
head -1 linux_commands.tab | sed 'y/abcdefghijklmnopqrstuvwxyz/ABCDEFGHIJKLMNOPQRSTUVWXYZ/; s/ /_/g'
```

### Ejemplos de uso básico de <b>sed</b>: **borrado de líneas: '<integer>d'**
- Borra la 1ª línea de archivo:
```
$ sed '1d' archivo
```
Ejemplo:
```{bash}
head -5 linux_commands.tab
echo '------------------------------------------------'
sed '1d' linux_commands.tab | head -4
```

- Elimina las líneas en blanco. Nótese el uso de <b>expresiones regulares</b>, done:
  + <b>//</b> delimitan la expresión regular. Noten que hay que escaparla entre 'comillas sencillas'.
  + <b>^</b> indica el inicio de la línea
  + <b>$</b> indica el término de la línea
  
```
$ sed '/^$/d' archivo
```

- Genera una lista numerada de los nombres de campos o cabeceras del archivo linux_commands.tab
  + <b>//</b> delimitan la expresión regular. Noten que hay que escaparla entre commillas sencillas.
  + <b>\\t</b> representa al tabulador
  + <b>\\m</b> representa el salto de línea
  + <b>//g</b> la <b>g</b> indica que se reemplacen todas las instancias


```{bash}
head -1 linux_commands.tab | sed 's/\t/\n/g' | cat -n
```

### Tutorial extenso de $sed$
Esta fue una presentación minimalista de $sed$, que no le hace justicia. Les recomiendo mucho el siguiente tutorial para profundizar:

[grymoire's sed tutorial](https://www.grymoire.com/UNIX/Sed.html)

***

# Salida estándar o standard output **STDOUT**
Como se explicó en el [tutoral de introducción a GNU/Linux](https://github.com/vinuesa/intro2linux/blob/master/docs/intro_biocomputo_Linux.pdf), la salida estándar $STDOUT$ de un comando o programa generalmente está ligada a la consola, es decir, vemos en la pantalla la salida del mismo. 

## Imprimir texto a STDOUT con **echo**

Para imprimir texto simple a la consola o archivo usa el comando $echo$

```{bash}
echo introduzca nombre de usuario:
```

- $echo$, como su nombre indica, simplemente imprime los argumentos que se le pasan. Por ello colapsa multiples espacios, ya que los toma como separadores de argumentos

```{bash}
echo vea    como     se    ignoran      múltiples espacios, colapsándolos a uno!
```

- Usa comillas sencillas o dobles si quieres preservar espaciado
```{bash}
echo '$USER: usa comillas sencillas para interpetación literal    de caracteres, incluídos espacios'
```

- Usa comillas dobles si quieres preservar espaciado y además interpolar variables. Nota el uso de **diagonal invertida \** para escapar el símbolo de pesos y evitar que se interpolen variables
```{bash}
echo "Hola $USER: usa comillas dobles para interpetación literal    de caracteres e interpolación de \$VARIABLES"
```

### Opciones más comunes de $echo$
Siempre puedes consultar las opciones de echo con el comando $man\ echo$. Pero te muestro algunas de las más comunes:

- imprimir texto separado por tabuladores usando la opción **-e** y **\t**
```{bash}
echo -e "texto\tseparado\tpor\ttabuladores"
```

- imprimir texto sin salto de línea al final usando la opción **-n**
```{bash}
echo -ne "este texto no lleva salto de línea al final,"; echo " pero esta línea sí!"
```

## Redireccionado de STDOUT a un archivo con el comando <b>></b>
Para redireccionar el STDOUT de un comando a archivo, necesitamos redirigir el flujo a dicho archivo con el comando $>$. 
```{bash}
echo -e "L1\nL2\L3" > tmp1.txt  # \n es el salto de línea; nota que si escribes sólo \, se imprime literalmente!
```

- Qué tipo de archivo genera el comando $>$ ?

```{bash}
file tmp1.txt
echo '-------------------'
ls -l tmp1.txt
```

- podemos desplegar el archivo con $cat$ u otros comandos como el paginador $less$
```{bash}
cat tmp1.txt
```


### Ejemplo de "vaciado" o desctrucción accidental de un archivo con **>**

Es importante entender que cada vez que se abre un nuevo descriptor de archivo con **>** se genera en primer lugar un archivo vacío, el cual es llenado después con la salida del comando. Eso se demuestra fácilmente con el siguiente ejemplo:

```{bash}
echo -e "L1\nL2\L3" > tmp1.txt
cat  tmp1.txt
```

Ahora veamos qué pasa si accidentalmente leemos y escribimos al mismo archivo
```{bash}
cat  tmp1.txt > tmp1.txt
```

En este caso, como el comando $cat$ primero genera el archivo tmp1.txt vacío, antes de leerlo. En el siguiente paso lee tmp1.txt, que es un archivo vacío, como demuestran los siguientes comandos
```{bash}
cat  tmp1.txt
ls -l tmp1.txt
```


## ¿Cómo indicar a un comando o programa que lea de un archivo y escriba a otro? - combinación de **<** y **>**

Muchas veces un programa debe leer un archivo para procesarlo e imprimir el resultado a un archivo. Esto es muy fácil en GNU/Linux, combinando **<** con **>**.

Para ello usa la sintaxis: <code>programa < archivo_a_leer > archivo_a_escribir</code>

Pero recuerden, no podemos leer y escribir al mismo archivo simultáneamente, como mostramos arriba.

```{bash}
echo -e "texto arbitrario\nmás texto\ny otra línea" > texto_arbitrario.txt
cat < texto_arbitrario.txt > borrame.txt

echo "imprimiendo texto_arbitrario.txt:" && cat texto_arbitrario.txt
echo '-----------------------------------------'
echo "imprimiendo borrame.txt:"; cat borrame.txt

echo
echo "borrando textos arbitrarios ..."
rm texto_arbitrario.txt borrame.txt

```

### Ejemplo de alineamiento generación de alineamiento múltiple con el alineador $muscle$
$muscle$ es un programa para hacer alineamientos múltiples. Por tanto no es una herramienta estándar de UNIX o GNU/Linux. Es un programa bioinformático que debermos descargar e instalar en nuestro sistema. Está instalado en tepeu. Lo usaremos aquí como ejemplo de llamadas del tipo <code>programa < archivo_a_leer > archivo_a_escribir</code>. Pasaremos la opción $-clw$ para escribir el alineamiento resultante en en formato *clustal*, más conveniente para su visualización en una terminal.

Nota: si quieres aprender más sobre alineamientos múltiples y alineadores, puedes consultar el [tutorial sobre alineamientos múltiples](https://github.com/vinuesa/TIB-filoinfo#sesi%C3%B3n-4-alineamientos-m%C3%BAltiples-teor%C3%ADa-y-pr%C3%A1cticas) que preparé para los [Talleres Internacionales de Bioinformática - 2019 (TIB19)](https://github.com/vinuesa/TIB-filoinfo).

```{bash}
# Noten el uso de los descriptores de archivo 1> y 2> /dev/null, para eliminar los mensajes de progreso de muscle
# Explicaremos descriptores de archivo en la siguiente sección
# podían haber escrito simplemente:
# muscle -clw < recA_Byuanmingense.fna > recA_Byuanmingense_muscle.aln
muscle -clw < recA_Byuanmingense.fna 1> recA_Byuanmingense_muscle.aln 2> /dev/null
```

Ahora veamos el primer bloque del alineamiento:

```{bash}
head -36 recA_Byuanmingense_muscle.aln
```

## Concatenación o pegado de salidas de varios comandos a un solo archivo con **>>**

- escribimos unas líneas a tmp1.txt
```{bash}
echo -e "L1\nL2\L3" > tmp1.txt
cat  tmp1.txt
```

- si no usamos un nombre de archivo de salida diferente, se sobreescribe tmp1.txt
```{bash}
echo -e "L4\nL5\L6" > tmp1.txt
cat  tmp1.txt
```

- podemos concatenar o pegar secuencialmene la salida de varios comando a un mismo archivo con **>>**
```{bash}
echo -e "L1\nL2\L3" > tmp1.txt
echo -e "L4\nL5\L6" >> tmp1.txt
cat  tmp1.txt

# limpiamos
rm tmp1.txt
```


***

# Descriptores de archivo:  0 == STDIN, 1 == STDOUT, 2 == STDERR
En GNU/Linux cada proceso típicamente inicia abriendo tres descriptores de archivo:  0 == STDIN, 1 == STDOUT, 2 == STDERR. Es responsabilidad del programador adherirse a estas convenciones, es decir, indicarle a los programas que escribe que imprima los resultados del programa a STDOUT y los posibles mensajes de error a STDOUT.

Copia y ejecuta los siguientes comandos en tu consola, para que lo entiendas. Le estamos pidiendo a 

-  aquí $ls$ intenta escribir su STDOUT a salida_no_existo.txt, pero no puede ya que el archivo no_existo.txt no existe en el directorio actual. 
```
$ ls no_existo.txt > salida_no_existo.txt
ls: cannot access 'no_existo.txt': No such file or directory
```
Lo que vemos arriba es el **mensaje de error** del comando $ls$ impreso a $STDOUT$, ya que no lo hemos redirigido a un archivo. 

```
$ ls -l salida_no_existo.txt # 0 bytes
-rw-rw-r-- 1 vinuesa vinuesa 0 oct 18 20:54 salida_no_existo.txt
```

## Uso de **1>**, **2>** y **&>** para redirgir $STDOUT$, $STDERR$ o ambos a un archivo

Esta llamada a $ls$ escribe su **STDOUT** al archivo stdout_no_existo.txt y su **STDERR** stderr_no_existo.txt.

```{bash}
ls no_existo.txt 1> stdout_no_existo.txt 2> stderr_no_existo.txt
ls -l stdout_no_existo.txt 
ls -l stderr_no_existo.txt
```

Cuando escribamos $scripts$ u otros programas, debemos manejar adecuadamente los descriptores de archivos para imprimir resultados y mensajes de error a sus archivos correspondientes. Si los queremos unir en uno solo, usaremos la sintaxis **&>**

```{bash}
ls no_existo.txt &> stdout_y_stderr_no_existo.txt
ls -l stdout_y_stderr_no_existo.txt

# limpiemos la basura
rm *existo.txt
```

## Eliminar mensajes impresos por un programa a STDERR con **2> /dev/null**

A veces no queremos ver los mensajes que imprime un protrama a STDOUT o STDERR, por ejemplo al correr un pipeline que llama a diversas aplicaciones, algunas de las cuales pueden imprimir copiosos mensajes de progreso del proceso a STDOUT.

```
# no imprime nada, ya que el mensaje a STDERR lo mandamos al "cubo de la basura"
ls no_existo.txt 2> /dev/null
```

- Veamos otros ejemplos muy ilustrativos.

```{bash}
head -2 recA_Bradyrhizobium_vinuesa.fna
```

```{bash}
head -2 recA_Bradyrhizobium_vinuesa.fna 1> /dev/null
```

```{bash}
head -2 recA_Bradyrhizobium_vinuesa.fna 2> /dev/null
```

```{bash}
head -2 recA_Bradyrhizobium_vinuesa.fna &> /dev/null
```

- Revisemos el ejemplo de alineamiento múltiple con $muscle$ que mostramos en la sección anterior usando:
  - Una llamada estándar a $muscle$, que imprime muchas líneas de mensaje de progreso a STDERR
```
# llamada estándar a muscle, que imprime a STDERR líneas documentando el progreso del programa ...
muscle -clw < recA_Byuanmingense.fna > recA_Byuanmingense_muscle.aln
```
  - una llamada enviando STDERR a /dev/null para eliminar los mensajes
```
# con uso de los descriptores de archivo 1> y 2> /dev/null, para eliminar los mensajes de progreso
muscle -clw < recA_Byuanmingense.fna 1> recA_Byuanmingense_muscle.aln 2> /dev/null
```

Nota: si quieres aprender más sobre alineamientos múltiples y alineadores, puedes consultar mi [tutorial sobre alineamientos múltiples](https://github.com/vinuesa/TIB-filoinfo#sesi%C3%B3n-4-alineamientos-m%C3%BAltiples-teor%C3%ADa-y-pr%C3%A1cticas)

******

# Trabajando con archivos: creación de tarros con **tar** y compresión **gzip**

Vimos en la sección anterior cómo leer archivos de texto comprimidos con compresión GNUzip, que convencionalmente llevan la extensión $.gz$.
En ésta veremos cómo comprimir y descomprimir archivos y directorios.

## Compresión gnuzip de archivos ASCII: comandos **gzip** y **gunzip**
El comando $gzip$ se usa principalmente para comprimir archivos con caracteres ASCII, y les agrega la extensión $.gz$ automáticamente.
Para descomprimir archivos $*.gz$ usamos el comando $gunzip$

- sintaxis $gzip$: <code>gzip file</code> o también <code>gzip *.gbk</code>
- sintaxis $gunzip$: <code>gunzip file.gz</code>

## Generación de 'tarros' comprimidos y extracción de archivos de ellos con el comando **tar**
Cuando necesitamos agrupar muchos archivos y comprimirlos, o comprimir un directorio y sus contenidos, el comando ideal es $tar$ (tape archive) que puede combinarse con gzip usando pipes, o más convenientemente aún, usando la opción -z, como se muestra abajo.

La convención es añadir la extensión **tgz**, o **tar.gz** a los tarros comprimidos con $gzip$.

1. Generación de tarros comprimidos
  - sintaxis $tar$ para múltiples archivos (genbank, por ejemplo): <code>tar -czf tarro_de_archivos_genbank_comprimidos.tgz *.gbk </code>
  - sintaxis $tar$ para archivos con compresión **bz2**: <code>tar -cjvf tarro_de_archivos_genbank_comprimidos.tar.bz2 *.gbk </code>
  - sintaxis $tar$ para un directorio: <code>tar -czvf tarro_de_directorio.tgz mi_directorio</code>
2. Extracción de tarros comprimidos
  - sintaxis para extraer archivos de un $tar.gz$: <code>tar -xzvf tarro_de_directorio.tgz</code>
3. Listar contenidos de tarros comprimidos con la opción **-t**
  - <code> tar -tzvf tarro_de_directorio.tgz</code>

Resumen de las opciones básicas de $tar$:

- -c create: **esta opción es obligatoria si se quiere crear un tarro**
- -x extract: **esta opción es obligatoria si se quiere extraer archivos de un tarro**
- -z use gzip compression. La compresión gnuzip estándar, con archivos terminados en $gz$
- -j use bz2 compression. Esta compresión es todavía más compacta, y los archivos resultantes clásicamente llevan la extensión $.bz2$
- -v verbose command output, es decir, va imprimiendo a STDOUT los archivos que va procesando
- -f file; **esta opción es obligatoria y tiene que ir al final del comando**
- -t  List the contents of an archive.

Es muy importante dominar $tar$ y $gzip$ y usarlos para:
- ocupar el mínimo espacio posible en disco, un recurso siembre limitado en un ambiente multiusuario
- comprimir archivos y directorios antes de moverlos entre máquinas mediante $scp$ o $sftp$
- poder desempacar y descomprimir distribuciones de código, las cuales casi invariablemente se distribuyen como tarros comprimidos.

Ejemplos:

- generación de un tarro comprimido

```{bash}
ls -l *fna                   # listamos los archivos *fna a empacar y comprimir
echo '----------------------------'
tar -czf fna_files.tgz *fna  # generamos un tarro comprimido con los archivos *fna
ls -l fna_files.tgz          # comprobamos que se generó el archivo
echo '----------------------------'
ls *fna                      # se empaca una copia de los archivos *fna, los cuales siguen existiendo sin comprimir
echo '----------------------------'
rm *fna                      # por ello los eliminamos con rm
```

- listar contenidos de un tarro comprimido
```{bash}
tar -tzf fna_files.tgz      # la opción -t permite listar el contenido de un tarro
```


- desempacado de un tarro comprimido
```{bash}
tar -xzf fna_files.tgz        # desempacamos y descomprimimos
ls -l *fna                    # vemos los *fna desempacados
echo '----------------------------'
ls -l fna_files.tgz           # vemos que sigue existiendo el *tgz
echo '----------------------------'
rm fna_files.tgz              # lo eliminamos
```

- generación de archivos con compresión gnuzip con $gzip$
```{bash}
ls -l *fna                    # listamos archivos *fna 
gzip *fna                     # comprimimos *fna
echo '----------------------------'
ls -l *fna*                    # vemos que ya no existen *fna, solo los *fna.gz, con compresión
```

- descompresión de archivos *.gz con $gunzip$
```{bash}
ls -l *fna*                  # listamos archivos *fna* (*fna.gz)
echo '----------------------------'
gunzip *fna.gz               # descomprimimos los *fna.gz
echo '----------------------------'
ls -l *fna*                   # vemos que ya no existen los archivos *fna.gz, solo los descomprimidos (*fna)
```

*****

# Tuberias de herramientas UNIX/Linux para filtrado de texto conectadas mediante "pipes" **|**
UNIX y GNU/Linux ofrecen una gran cantidad de herramientas para todo tipo de funciones o labores, cada una generalmente con muchas opciones. En bioinformática y genómica, los archivos de texto plano (ASCII) son los más comunes. Por ello es muy útil dominar al menos algunas de las herramientas de filtrado de texto que ofrece el entorno GNU/Linux. Como ejemplo, trabajaremos con el archivo assembly_summary.txt.gz, que contiene los datos de ensambles genómicos de la división RefSeq de GenBank. Lo descargué y comprimí en Julio de 2019 con los siguientes comandos:

```
# descargamos con el comando wget
wget -c ftp://ftp.ncbi.nlm.nih.gov/genomes/refseq/bacteria/assembly_summary.txt

# compresión gnu-zip del archivo
gzip assembly_summary.txt
```
- Exploremos el archivo comprimido (con compresión gnuzip) usando una combinación de los comandos <b>zcat</b> y <b>zless</b>
```
# con zcat mandamos el flujo de datos descomprimidos a less -L, 
#  que nos permite navegar esta tabla de muchas columnas horizontal y verticalmente
zcat assembly_summary.txt.gz | less -L
```

Veamos en detalle el uso del comando **|** y de algunas herramientas de filtrado trabajando sobre el archivo assembly_summary.txt

## El comando **|** (*pipe*:)
Como explicamos en el [tutoral de introducción](https://github.com/vinuesa/intro2linux/blob/master/docs/intro_biocomputo_Linux.pdf) y detallamos en la sección de [STDOUT](#salida-estándar-o-standard-output-stdout), el resultado de un comando ejecutado en la consola se despliega en la pantalla. Es decir, la salida estándar o **STDOUT** de un comando está asociada por defecto a la terminal o pantalla. 
Hemos podido comprobar también que cada comando hace por lo general una sola función, pero con muchas opciones. Las herramientas de GNU/Linux siguen fielmente este principio de modularidad, fundamental en la ingeniería de código. 

Lo genial del diseño de los sistemas operativos UNIX y GNU/Linux, es que las diferentes utilerías pueden combinarse según se requiera, lo cual confiere una gran versatilidad al usuario para diseñar soluciones específicas para esencialmente cualquier tarea o problema. Para ello se conecta la salida estándar de un comando (**STDOUT**) con la entrada estándar (**STDIN**) del siguiente mediante el comando **|** o *pipe*, siguiendo esta sintaxis:

<code>comando1 [-opcion1 ...] | comando2 [-opcion1 ...] | comando3 [-opcion1 ...] ...</code>

como se muestra en los siguientes ejemplos básicos, que ilustran el principio:

- cuenta el número de archivos en el directorio $/bin$

```{bash}
ls /bin | wc -l
```

- filtra los subdirectorios en tu $\$HOME$ que inician con 'D'

```{bash}
ls -d $HOME/*  | grep '/D'
```

La selección juiciosa de secuencias de comandos y sus opciones permite hacer operaciones muy específicas y de complejidad arbitraria

## Ejemplos de herramientas de filtrado de texto: <b>cut</b>, <b>grep</b>, <b>sort</b>, <b>uniq</b>, <b>wc</b> y <b>|</b> en acción
- $cut$ corta líneas de texto/tablas por delimitadores de campo (-d) específicos (TAB por defecto), extrayendo los campos indicados con -f (cut -d' ' -f1-3,5,9)
- $sort$ ordena (sort -u; sort -nrk2; sort -dk1)
- $wc$ cuenta líneas, palabras y caracteres (wc -l)
- $uniq$ regresa listas de valores únicos (uniq -c)
- $grep$ Filtra las lineas de un archivo que contienen (o no) caracteres o expresiones regulares (grep -E '^XXX|YYY|zzz$'; grep -v '^#')
- el pipe '|' conecta la salida de un comando <STDOUT> con la entrada <STDIN> de otro

- ¿cuántas líneas tiene el archivo assembly_summary.txt.gz?
```{bash}
# ¿cuántas líneas tiene el archivo assembly_summary.txt.gz?
zcat assembly_summary.txt.gz | wc
zcat assembly_summary.txt.gz | wc -l
```

- la columna assembly_level (#12) indica el estado del ensamble. ¿Cuáles son los niveles de la variable categórica assembly_level (valores únicos de la misma?
```{bash}
# la columna assembly_level (#12) indica el estado del ensamble. ¿Cuáles son los niveles de la variable categórica assembly_level (valores únicos de la misma?
zcat assembly_summary.txt.gz | grep -v "^#" | cut -f 12 | sort -u
```

- ¿cuántos genomas hay por nivel de la variable categórica assembly_level?
```{bash}
# ¿cuántos genomas hay por nivel de la variable categórica assembly_level?
#   noten que en este ejemplo usamos tail -n +2 para evitar la primera línea
#   sed '1d' hubiese funcionado igualmente y es el comando más corto que conozco para ello
zcat assembly_summary.txt.gz | tail -n +2 | cut -f 12 | sort | uniq -c
```

- asocia cada nombre de columna de la cabecera con el número de la columna correspondiente
```{bash}
# asocia cada nombre de columna de la cabecera con el número de la columna correspondiente
zcat assembly_summary.txt.gz | head -2 | sed '1d; s/\t/\n/g' | cat -n
```

- genera una estadística del número de genomas por especie (columna # 8), y muestra sólo las 10 especies con más genomas secuenciados!
```{bash}
# genera una estadística del número de genomas por especie (columna # 8), y muestra sólo las 10 especies con más genomas secuenciados!
zcat assembly_summary.txt.gz | grep -v "^#" | cut -f8 | sort | uniq -c | sort -nrk1 | head -10
```

- ¿Cuántos genomas completos hay del género Acinetobacter?
```{bash}
# ¿Cuántos genomas completos hay del género Acinetobacter?
zcat assembly_summary.txt.gz | grep Acinetobacter | grep Complete | wc -l

# también puedes usar zgrep para evitar la llamada primero a zcat
zgrep Acinetobacter assembly_summary.txt.gz | grep Complete | wc -l
```

```
# Ojo: recuerda que GNU/Linux es sensible a mayúsculas y minúsculas: prueba este comando para comprobarlo
zgrep acinetobacter assembly_summary.txt.gz | grep Complete | wc -l # no encuentra nada
```

```{bash}
# grep -i lo hace insensible a la fuente
zgrep -i acinetobacter assembly_summary.txt.gz | grep Complete | wc -l
```

- filtra y cuenta las lineas que contienen Acinetobacter o Stenotrophomonas
```{bash}
# filtra y cuenta las lineas que contienen Acinetobacter o Stenotrophomonas
zgrep -E 'Acinetobacter|Stenotrophomonas' assembly_summary.txt.gz | wc -l
```

- Cuenta los genomas de *Acinetobacter*, *Pseudomonas* y *Klebsiella* (por género) y presenta una lista ordenada por número decreciente de genomas
```{bash}
# Cuenta los genomas de Acinetobacter, Pseudomonas y Klebsiella (por género) y presenta una lista ordenada por número decreciente de genomas
zgrep -E 'Acinetobacter|Pseudomonas|Klebsiella' assembly_summary.txt.gz | cut -f 8 | cut -d' ' -f1 |sort -d | uniq -c |sort -nrk1
```

- NOTA: vean quen tenemos entradas en la tabla de '[Pseudomonas]'. Deberíamos de estudiar esos genomas con más detalle para tomar una decisión informada sobre qué hacer con ellos: eliminarlos o incluirlos en la tabla de resultados con el resto de las *Pseudomonas*. Hay un genoma que contiene el término *Candidatus*, que se usa para genomas reconstituidos de estudios metagenómicos, es decir, para los cuales no existe un aislamiento en cultivo. Pueden filtrar y revisar esas entradas de la tabla con <code>zgrep '\[Pseudomonas\]' assembly_summary.txt.gz | less -L</code> y tomar la decisión. Van dos soluciones de código para cada caso:
  - incluirlas con las otras *Pseudomonas* y eliminar la entrada de *Candidatus*
```{bash}
zgrep -E 'Acinetobacter|Pseudomonas|Klebsiella' assembly_summary.txt.gz | cut -f8 | cut -d' ' -f1 | sed 's/\[//; s/\]//' | grep -v 'Candidatus' | sort -d | uniq -c | sort -nrk1
```
  - descartar las entradas de [Pseudomonas] y *Candidatus*
```{bash}
zgrep -E 'Acinetobacter|Pseudomonas|Klebsiella' assembly_summary.txt.gz | cut -f8 | cut -d' ' -f1 | grep -Ev '\[|Candidat' | sort -d | uniq -c | sort -nrk1
```

-  Cuenta los genomas de *Acinetobacter*, *Pseudomonas* y *Klebsiella* (por género), con salida ordenada alfabéticamente por género
```{bash}
# filtra las lineas que contienen Filesystem o Text processing y ordénalas alfabéticamente según las entradas de la segunda columna
# eliminando las entradas de Candidatus y [Pseudomonas]
zgrep -E 'Acinetobacter|Pseudomonas|Klebsiella' assembly_summary.txt.gz | cut -f 8 | cut -d' ' -f1 | grep -Ev '^\[|^Cand' | sort | uniq -c | sort -dk2
```

Veremos la gran utilidad y versatilidad de combinaciones de estos comandos para el procesamiento de archivos de secuencias en un ejercicio integrativo posterior.

*****

# El lenguaje de procesamiento de patrones <b>AWK</b> y su sucesor GNU <b>gawk</b>
<b>AWK</b> es un lenguaje de programación diseñado para procesar datos de texto, ya sean ficheros o flujos de datos. El nombre <b>AWK</b> deriva de las iniciales de los apellidos de sus autores: Alfred <b>A</b>ho, Peter <b>W</b>einberger, y Brian <b>K</b>ernighan. <b>$awk$</b>, cuando está escrito todo en minúsculas, hace referencia al programa de UNIX que interpreta programas escritos en el lenguaje de programación AWK. Es decir, <b>AWK</b> es un lenguaje interpretado por el intérprete de comando <b>$awk$</b>. 

<b>AWK</b> fue creado como un reemplazo a los algoritmos escritos en C para análisis de texto. Fue una de las primeras herramientas en aparecer en UNIX (en la versión 3). Ganó popularidad rápidamente por la gran funcionalidad permitía añadir a las tuberías de comandos de UNIX. Por ello se considera como una de las utilidades necesarias (*core*) de este sistema operativo. 

$awk$ fue portado originalmente al proyecto GNU por Paul Rubin en 1986, llamándolo $gawk$. Desde entonces $gawk$ ha ganado mucha funcionalidad adicional, siendo actualmente **Arnold Robins** el principal mantenedor del código y de su extraordinaria [documentación](https://www.gnu.org/software/gawk/manual/gawk.html), la cual te recomiendo consultes asiduamente cuando estés aprendiendo **AWK**

En sistemas modernos de GNU/Linux la llamada al intérprete de comandos $awk$ está ligada a $gawk$, como podemos ver en las siguientes salidas de una máquina que corre *ubuntu 20.04.1*. 

- veamos qué versión imprime $awk$
```{bash}
awk --version | head -2
```

- ¿dónde vive en el sistema?
```{bash}
which awk
```

- ¿qué tipo de archivo es awk?
```{bash}
ls -l /usr/bin/awk
```

- ¿a dónde apunta esta liga simbólica?
```{bash}
ls -l /etc/alternatives/awk 
```

- ¿y a dónde apunta la siguiente?
```{bash}
ls -l /usr/bin/gawk
```

- y para terminar de convencernos, veamos qué versión imprime $gawk$
```{bash}
gawk --version | head -2
```

Por tanto, las llamadas que veremos a $awk$ de los ejemplos que siguen en realidad ejecutan $gawk$.

$gawk$, cuya versión más reciente es la 5.1, es la variante de los "**nuevos awks** o $nawk$" derivados del viejo $AWK$ con más funcionalidad integrada, como pueden consultar en la excelente [guía del usuario de GAWK](https://www.gnu.org/software/gawk/manual/gawk.html).

## Conceptos fundamentales sobre la estructura, variables internas y funcionamiento de los programas AWK
Debido a su densa notación, lenguajes como $awk$ son frecuentemente usados para escribir *programas de una línea* o ("**one-liners**"), como veremos seguidamente.

En general, al intérprete de comandos $awk$ se le pasan dos piezas de datos: 

- <b>un fichero de órdenes o programa</b> 
- <b>uno o más archivos de entrada</b>. 

Un fichero de órdenes (que puede ser un fichero real, o puede ser incluido en la invocación de $awk$ desde la línea de comandos) contiene una serie de sentencias que le indican a $awk$ cómo procesar el fichero de entrada. Es decir contiene el **programa** escrito en sintaxis **AWK**.

- <code>$ awk 'programa_AWK' archivo1 archivo2 ...</code>
- <code>$ awk -f 'archivo_con_código_AWK' archivo1 archivo2 ...</code>

El fichero primario de entrada es normalmente texto estructurado con un formato particular, por defecto archivos con campos separados por espacios o tabuladores (tablas).

### $FILENAME$
Si se especifican varios archivos, éstos se procesan en el orden en el que se le pasan a $awk$. El nombre del archivo que está siendo procesado por $awk$ queda guardado en la variable interna **FILENAME**.

### **Registros** las variables asociadas **$RS$**, **$FNR$** y **$NR$**
$awk$ procesa los archivos en unidades conocidas como **registros** (*records*), que se procesan acorde a las órdenes del *programa*, registro por registro.

**Por defecto, los registros se separan con saltos de línea** (<code>RS="\n"</code>). Es decir, por defecto $awk$ modela tablas, en las que cada fila corresponde a un registro.

- estructura de un archivo con estructura tabular, donde cada fila corresponde a un registro
```{bash}
cat mini_tabla.tsv
```

- El **separador de registros**, guardado en la variable interna $RS$ (*Record Separator*) se puede modificar asignándole un valor que puede ser una expresión regular. Ello le permite al programador **modelar con gran precisión y versatilidad la estructura de los registros**. 

Así por ejemplo, si nuestros datos están contenidos en un archivo de *secuencias multifasta*, puede ser muy conveniente definir el separador de registros de la siguiente manera: <code>RS=">"</code>, ya que cada nuevo registro (secuencia en este caso) inicia con el símbolo '>'

- La variable interna **FNR** cuenta el número de registros del archivo en procesamiento. Esta variable se reinicia a 0 cuando $awk$ va a procesar un nuevo archivo.
- La variable interna **NR** cuenta el número de registros que ha procesado $awk$ de manera acumulativa (nunca se reinicia si lee un nuevo archivo)

Veamos un primer ejemplo de código AWK que ilustra lo arriba descrito en relación a las variables $FILENAME$, $RS$, $FNR$ y $NR$.

- Visualicemos primero la estructura y contenido de los dos archivos que le vamos a pasar a $awk$
```{bash}
echo " >>> contenido del archivo seq.list <<<"
cat seq.list 
echo
echo ">>> contenido del archivo mini_fasta.fst <<<"
cat mini_fasta.fst
```

- Sigue el mini programa de AWK ('encerrado en comillas sencillas') que simplemente procesa los dos archivos que le pasamos como argumentos al final de programa, imprimiendo el contenido de las variables FILENAME, FNR, NR que $awk$ automáticamente pone a disposición del programador
```{bash}
awk  '{print FILENAME, FNR, NR}' seq.list mini_fasta.fst
```

¿Qué está haciendo el programa? Dado que por defecto <code>RS="\n"</code>, $awk$ está leyendo cada uno de los dos archivos secuencialmente, línea por línea, imprimiendo los valores actualizados de las variables FILENAME, FNR, NR.

- El siguiente programa imprime también los valores de $RS$ y $FS$, que por defecto son "RS=\n" y "FS=[[:space:]]+", que es una $regexp$ que encuentra espacios, tabuladores y saltos de línea. 
```{bash}
awk  '{print FILENAME, FNR, NR, "["RS"]", "["FS"]"}' seq.list mini_fasta.fst
```

Define exactamente la salida anterior en función del programa.

### Los **campos** de un registro y las variables $FS$, $NF$, *$0* y *$1 ... $n*
Por defecto $awk$ procesa automáticamente cada registro (acorde al valor asignado a $RS$), separándolo en **campos** delimitados por espacios, tabuladores o saltos de línea, como indica la siguiente expresión regular: "FS=/[[:space:]]+/" o su equivalente "FS=/\\s+/", como se muestra en los siguientes ejemplos:

#### ¿Cómo afecta la variables **RS** a la definición de un campo?

```{bash}
echo "1blablabla"; echo -e "1blablabla" | awk '/[[:space:]]+/ {print " ==> regexp machea!"}'
echo "2bla bla bla"; echo -e "2bla bla bla" | awk '/[[:space:]]+/ {print " ==> regexp machea!"}'
echo "3bla\tbla\tbla\t"; echo -e "3bla\tbla\tbla" | awk '/[[:space:]]+/ {print " ==> regexp machea!"}'
echo ">4blablabla"; echo -e ">4blablabla" | awk 'BEGIN{RS =">"} /[[:space:]]+/ {print " ==> regexp machea!"}'
echo ">5blablabla"; echo -ne ">5blablabla" | awk 'BEGIN{RS =">"} /[[:space:]]+/ {print " ==> regexp machea!"}'
```

- Asegúrate de entender la salida mostrada arriba
  - ¿Porqué la *regexp* no coincide con la cadena en el ejemplo 1 y sí lo hace en los ejemplos 2 y 3?
  - ¿Porqué la *regexp* no coincide con la cadena en el ejemplo 1 y sí lo hace en el ejemplo 4?
  - ¿Porqué la *regexp* no coincide con la cadena en el ejemplo 5 y  sí lo hace en el ejemplo 4?


#### Variables  **$0, $1, ... $n**

- Cada campo del registro en procesamiento es guardado en las variables internas **$1 ... $n**

- El contenido del registro completo está guardado en **$0** y el número total de sus campos en **NF**

Noten que las variables **$0, S1 ... $n** son las únicas en AWK que siempre van precedidas del símbolo *\$*.

- Define exactamente qué está haciendo el programa, es decir, interpreta o explica con precisión la salida observada
```{bash}
awk  '{print FILENAME, FNR, NR, $1, $0}' seq.list mini_fasta.fst
```

- ¿Qué estructura tiene el siguiente archivo y cuáles piensas que serían los valores ideales de las variables $RS$ y $FS$ para modelarla en $awk$?
```{bash}
cat mini_tabla.tsv
```
- Define con precisión qué está haciendo el siguiente programa acorde a la salida observada
```{bash}
awk  '{print FILENAME, NR, $1, $NF}' mini_tabla.tsv
```
  1. Con respecto a la salida mostrada arriba, ¿piensan que refleja adecuadamente lo que pretendía el programador? 
  2. ¿Qué mejora debería hacerse en el modelado de la estructura del registro para un procesado adecuado?

### Ejemplos genéricos de estructura de programas AWK

- Un programa AWK típico consiste en una serie de líneas, cada una de la forma:

<code>/patrón/ { acción }</code>, donde:

- la **acción por defecto** es imprimir <code>{print}</code>
- patrón es una <b>[expresión regular](https://es.wikipedia.org/wiki/Expresi%C3%B3n_regular)</b>, como explicaremos con mayor detalle en una sección posterior
- **acción** es una orden o programa, que puede ser todo un *script* te AWK de cientos o miles de líneas

$awk$ lee línea por línea el fichero de entrada. Cuando encuentra una línea que coincide con el **patrón**, ejecuta la(s) orden(es) o programa indicadas en **acción**. 

- Para llamar a $awk$ desde la línea de comandos, usaríamos una sintaxis de este tipo:

<code>awk 'CODIGO AWK' ARCHIVO_A_PROCESAR</code>

- para usarlo en una tubería de $Shell$, conecta el $STDOUT$ de un programa al $STDIN$ de $awk$ mediante **|**:

<code>programaX | awk 'CODIGO AWK' > output_file.txt</code>

Vuelve a examinar la estructura del archivo mini_tabla.tsv y define con precisión qué están haciendo los siguientes programas, acorde a la salida que imprimen
1. 
```{bash}
awk  '/UNAM/' mini_tabla.tsv
```
2. 
```{bash}
awk  'NR == 2' mini_tabla.tsv
```
3. 
```{bash}
awk  'NF > 9' mini_tabla.tsv
```

### Formas alternativas del código AWK y uso de bloques **BEGIN{}**, **END{}**

<code>BEGIN { acción }</code> Ejecuta las órdenes de acción al comienzo de la ejecución, antes de que los datos comiencen a ser procesados. Aquí inicializamos variables globales como RS, FS, OFS ...
   
<code>END { acción }</code> Similar a la forma previa, pero ejecuta las órdenes de acción después de que todos los datos sean procesados, por ejemplo para imprimir estadísticas de resumen después de haber analizado los campos de cada registro.
    
<code>/patrón/</code> Imprime las líneas que contienen al patrón.
    
<code>{ acción }</code> Ejecuta acción por cada línea en la entrada.

Cada una de estas formas pueden ser incluidas varias veces en un archivo o $script$ de $AWK$. El $script$ es procesado de manera progresiva, línea por línea, de izquierda a derecha. Entonces, si hubiera dos declaraciones $BEGIN$, sus contenidos serán ejecutados en orden de aparición. Las declaraciones $BEGIN$ y $END$ no necesitan estar en forma ordenada.

## Inicialización de variables en bloque BEGIN{} para modelar adecuadamente la estructura de un registro
Como indicábamos anteriormente, $AWK$ permite al programador **modelar con gran precisión y versatilidad la estructura de los registros** mediante la asignación de los valores más apropiados a las variables $RS$, y $FS$. Esto se hace generalmente dentro de un **bloque de inicialización BEGIN{}**, como se muestra en los ejemplos que siguen.

Revisemos nuevamente algunos de los ejemplos usados anteriormente con los valores por defecto de $RS$, $FS$ y $OFS$.

### Análisis de archivos con campos separados por tabuladores (tsv)

1. Mejorando el modelado de la estructura de los registros del archivo *mini_tabla.tsv*
```{bash}
head -5 mini_tabla.tsv
```
  - En base a la salida anterior, ¿cuál piensas que era la intención del programador y qué piensas está mal en el código que sigue?
```{bash}
awk '{print $1, $2, $NF}' mini_tabla.tsv
```

Obviamente debemos cambiar el valor por defecto de <code>FS=[[:space:]]+</code>, *regexp* que se ajusta a uno o más espacios, tabuladores y saltos de línea a esta otra <code>FS="\t"</code> qué sólo empareja con tabuladores.
```{bash}
awk 'BEGIN{FS="\t"} {print $1, $2, $NF}' mini_tabla.tsv
```
La salida del programa anterior la podemos mejorar indicando que el **separador de campo de la salida** $OFS$ sea también un tabulador, para respetar así la estructura original del archivo de entrada.
```{bash}
awk 'BEGIN{FS="\t"; OFS=FS} {print $1, $2, $NF}' mini_tabla.tsv
```

Excelente, ya hemos modelado adecuadamente un archivo tabular. Ahora nuestro código AWK responderá adecuadamente a nuestras intenciones, es decir, la *sintaxis* permite modelar la estructura de los datos para que se ajuste a la *semántica* del programa.
```{bash}
awk 'BEGIN{FS="\t"; OFS=FS} /Pseudomonas/ {print $1, $2, $NF}' mini_tabla.tsv
```

### Análisis de archivos FASTA - registros delimitados por '>'
Veamos ahora cómo modelar de manera más adecuada un archivo FASTA, para lo cual volveremos a trabajar con *mini_fasta.fst*. Visualicemos su estructura:

```{bash}
cat  mini_fasta.fst
```

- ¿qué intención crees que tenía el programador al escribir la siguiente línea de código y 
qué problema tiene el programa que impide obtener el resultado esperado?
```{bash}
awk '/seq3/' mini_fasta.fst
```

En base a la estructura de un archivo FASTA, ¿cómo piensas que debería de modelarse en AWK?

El cambio más obvio e importante sería indicarle a la variable de **separador de registro RS** que éstos vienen delimitados por un '>'.
```{bash}
awk 'BEGIN{RS=">"} /seq3/' mini_fasta.fst
```

- Compara e interpreta las diferencias de las salidas de los programas que siguen:
  -  esta es la salida con los valores por defecto de $RS$ y $FS$
```{bash}
awk ' {print NR, NF, $1, $0}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$, imprimiendo campo 1
```{bash}
awk 'BEGIN{RS=">"} {print NR, NF, $1}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$, imprimiendo campos 1 y 2
```{bash}
awk 'BEGIN{RS=">"} {print NR, NF, $1, $2}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$, imprimiendo campos 1, 2 y 3
```{bash}
awk 'BEGIN{RS=">"} {print NR, NF, $1, $2, $3}' mini_fasta.fst
```

  - esta es la salida con $RS=">"$, imprimiendo el registro completo
```{bash}
awk 'BEGIN{RS=">"} {print NR, NF, $0}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$ y $FS="\n"$, imprimiendo campo 1
```{bash}
awk 'BEGIN{RS=">"; FS="\n"} {print NR, NF, $1}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$ y $FS="\n"$, imprimiendo campo 2
```{bash}
awk 'BEGIN{RS=">"; FS="\n"} {print NR, NF, $2}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$ y $FS="\n"$, imprimiendo campos 1, 2 y 3
```{bash}
awk 'BEGIN{RS=">"; FS="\n"} {print NR, NF, $1, $2, $3}' mini_fasta.fst
```

  - esta es la salida con $RS=">"$ y $FS="\n"$, imprimiendo el registro completo
```{bash}
awk 'BEGIN{RS=">"; FS="\n"} {print NR, NF, $0}' mini_fasta.fst
```
  - esta es la salida con $RS=">"$ y $FS="\n"$, imprimiendo el registro completo para registros mayores al primero ($NR>1$)
```{bash}
awk 'BEGIN{RS=">"; FS="\n"} NR > 1 {print NR, NF, $0}' mini_fasta.fst
```

Si quieres dominar $awk$, es crítico que sepas manejar las variables  $NR$, $FS$, $OFS$ adecuadamente. Los ejemplos precedentes, si los estudias con cuidado, deben darte un buen nivel de comprensión de su comportamiento por defecto y cómo manipularlas para modelar adecuadamente la estructura de los registros (datos).  

Revisado en detalle este aspecto clave, podemos pasar a aprender los elementos fundamentales del lenguaje AWK.

## Sintaxis condensada de AWK
AWK no es sólo una herramienta de filtrado de texto estructurado. Es un **lenguaje de programación completo**, con estructuras de control y flujo, diversos operadores y funciones integradas para trabajar con cadenas de caracteres o con números, controlar el formato de la salida impresa, estructuras de datos y posibilidad de escribir funciones *ad hoc*. 

### Condicionales, bucles, operadores boleanos y relacionales, funciones integradas y definidad por el usuario, arreglos
$gawk$, al ser un lenguaje de programación completo, contiene sintaxis para escribir:

- **condicionales** <code>if(condicion1){code1}else if(condición2){code2}else{code3}</code> 
- **bucles for** <code>for (i in array){code}</code>; <code>for(initialization;condition;increment|decrement)</code>
- **bucles while** <code>wile(true){code}</code> 
- **operadores aritméticos** <code>+, -, *, /, %, =, ++, --, +=, -=, ...)</code>
- **operadores boleanos** <code>||, &&</code>
- **operadores relacionales** <code><, <=, == !=, >=, ></code>
- **funciones integradas**: <code>length(str); int(num); index(str1, str2); split(str,arr,del); substr(str,pos,len);<br/>
     printf(fmt,args); tolower(str); toupper(str); gsub(regexp, replacement [, target])</code>
- **funciones escritas por el usuario** <code>function FUNNAME (arg1, arg1){code}</code>
- **estructuras de datos** ($hashes$ o $arreglos\ asociativos$): <code>array[string]=value</code>. 

Esta lista no es exhaustiva en absoluto, pero contiene muchos de los elementos del lenguaje más frecuentemente usados. 
En los ejemplos que siguen en el resto de la sección de $AWK$ veremos implementaciones prácticas de la mayoría de éstos, 
explicando su sintaxis a medida que aparezcan.

### Resumen de variables internas más usadas:
La mayoría de las variables internas listadas seguidamente ya las explicamos y vimos en acción en ejemplos precedentes. 
Las presento abajo en un solo bloque para facilitar su referenciado.

<pre>
$0       guarda el valor de la fila actual en memoria de un archivo de entrada
$1-$n    guarda los contenidos de los campos de una fila
ARGC     variable que guarda el número de argumentos (+1) pasados al script desde la línea de comandos, después del bloque de código
         NOTA: ARGC contiene siempre un valor más que argumentos pasados al script debido a  que implícitamente define ARGV[0]=awk 
ARGV     arreglo que guarda los argumentos pasados al script desde la línea de comandos, después del bloque de código. ARGV[0] contiene awk
FILENAME nombre del archivo de entrada actualmente en procesamiento
FS       (Field Separator) separador de campos (por defecto SPACE or TAB)
NR       (Number or Records) guarda el número de campos delimitados por FS en registro o fila actual
OFS      (Output Field Separator) separador de campo de la salida (SPACE por defecto)
ORS      (Output Return Separator) separador de registro de la salida (\n por defecto)
</pre>

## Expresiones regulares - una breve introducción 
**Una expresión regular** (*regex*) define un conjunto o más de cadenas de caracteres. 
Una cadena literal de caracteres es una *regex* que define a una sola cadena: a sí misma. 
Una expresión regular más compleja, usando la notación adecuada, que incluye letras, 
números (**caracteres ordinarios**) y una gran cantidad de caracteres adicionales (**caracteres especiales** o **metacaracteres**), 
puede definir a un conjunto más amplio, pero específico, de cadenas de caracteres. 

- Los **caracteres ordinarios** usados en *regexes son: <code> _, A-Z, a-z, 0-9</code>
- Los **metacaracteres** usados en *regexes* son: <code>.*[]^${}\+?|()</code>

Las *regexes* se utilizan para encontrar patrones específicos en archivos de texto. 
Programas como $ed$, $vim$, $grep$, $sed$,  $awk$, $perl$, entre otros muchos, 
hacen uso de *regexes* para buscar dichos patrones en archivos de texto. 
Cuando los encuentra, se dice que la *regexp* encontró un *match* o concordancia.

Hay diversas implementaciones del lenguaje de expresiones regulares. Aquí sólo veremos las *regexes* del estándar *POSIX* que usan la mayoría de los programas de *GNU/Linux*.

Además, hay dos motores de búsqueda de patrones mediante expresiones regulares:

- el motor Básico de Expresiones Regulares (**BRE**) por sus siglas en inglés, usado por ejemplo por $sed$ y $grep$
- el motor Extendido de Expresiones Regulares (**ERE**), usado por ejemplo por $gawk$ y $perl$

### Delimitadores de expresiones regulares **//** y búsqueda de coincidencias
Los delimitadores estándar son **/regex/**. 

En $gawk$ buscamos coincidencias de la *regexp* en líneas de un archivo o campos de una línea con sintaxis de este estilo:

- <code>awk '$1 == "cadena" { print $2 }' archivo</code>          # imprime campo dos si campo uno es igual a *cadena*
- <code>awk '$1 ~ "/regex/" { print $2 }' archivo</code>          # imprime campo dos si campo uno concuerda con *regex*
- <code>awk '$1 !~ "/regex/" { print $2 }' archivo</code>         # imprime campo dos si campo uno NO concuerda con *regex*
- <code>awk '/cadena/ { print $0 }' archivo</code>                # imprime toda la línea si concuerda con cadena
- <code>awk '/cadena1/ && !/cadena2/ { print $0 }' archivo</code> # imprime toda la línea si concuerda con cadena1 y no con cadena2

### Notación de caracteres especiales más frecuentemente usados

| Notación    | Significado                                                        | Motor BRE/ERE  |
| ------------|:------------------------------------------------------------------:|:--------------:|
| \           | escapa el significado del metacaracter, interpretación literal     | AMBAS          |
| .           | cualquier caracter sencillo salvo NULL                             | AMBAS          |
| *           | machea cualquier cantidad de veces (o cero) el caracter precedente | AMBAS          |
| ^           | machea la regexp al inicio de la línea o cadena de caracteres      | AMBAS          |
| $           | machea la regexp al final de la línea o cadena de caracteres       | AMBAS          |
| [123] [A-Z] | machea cualquiera de los caracteres incluidos o rango indicado     | AMBAS          |
| {n,m}       | expresión de intervalo: machea n instancias, o de n a m instancias | ERE            |
| +           | machea una o más instancias de la regexp precedente                | ERE            |
| ?           | machea cero o una instancia de la regexp precedente                | ERE            |
| \|          | machea regexp especificada antes o después de \| (esto\|aquello)   | ERE            |
| ()          | busca un match al grupo de regexes incluidas:  (esto\|aquello)     | ERE            |


### Notación POSIX de clases de caracteres BRE frecuentemente usados

| Notación    | Significado                                                        | Motor BRE/ERE  |
| ------------|:------------------------------------------------------------------:|:--------------:|
| [[:alnum:]] | alfanuméricos [a-zA-Z0-9_]                                         | AMBAS          |
| [[:alpha:]] | carcteres alfabéticos [a-zA-Z]                                     | AMBAS          |
| [[:space:]] | espacios (' ', tabuladores, formfeed)                              | AMBAS          |
| [[:blank:]] | machea espacios y tabuladores                                      | AMBAS          |
| [[:upper:]] | machea [A-Z]                                                       | AMBAS          |
| [[:lower:]] | machea [A-Z]                                                       | AMBAS          |
| [[:digit:]] | machea [0-9]                                                       | AMBAS          |

### Notación POSIX de clases de caracteres ERE frecuentemente usados

| Notación     | Significado                                          | Motor BRE/ERE  |
| ------------ |:---------------------------------------------------- |:-------------- |
| \\w          | caracteres alfanuméricos [a-zA-Z0-9_]                | ERE            |
| \\W          | caracteres NO alfanuméricos [^[:alnum:]_]            | ERE            |
| \\s          | machea espacios y tabuladores                        | ERE            |
| \\d          | machea [0-9]                                         | ERE            |
| \\<          | machea el inicio de una palabra                      | ERE            |
| \\>          | machea el final de una palabra                       | ERE            |


### Un ejemplo de expresión regular compleja: validación de un correo electrónico
```{bash}
echo "usuario@entidad.unam.mx" | awk '/^([a-zA-Z0-9_\-\.\+]+)@([a-zA-Z0-9_\-\.]+)\.([a-zA-Z]{2,5})$/ {print $0}'
```

### Manipulación de cadenas de caracteres con las funciones **sub()**, **gsub()**, **substr()** y expresiones regulares
$AWK$ provee de diversas funciones para la manipulación de cadenas de caracteres. Les voy a mostrar sólo tres funciones muy sencillas de usar:

-**sub()** <code>/regexp/, "reemplazo", [, diana] </code>
-**gsub()** <code>/regexp/, "reemplazo", [, diana]</code>
-**substr()** <code>cadena, inicio, [, longitud]</code>

$sub()$ busca la instancia más larga en la cadena diana de la $regexp$, sustituyéndola por la cadena reemplazo. La cadena diana así modificada pasa a ser el nuevo valor de diana. 
Es importante resaltar que *la cadena diana* tiene que estar guardada en una variable. 

$sub()$ sólo reemplaza la primera instancia de $regexp$, mientras que $gsub()$ realiza reemplazos globales (tipo: *s/foo/bar/g*)

- $sub()$
```{bash}
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; print str; sub(/cadena/, "secuencia", str); print str }'
```
- $gsub()$
```{bash}
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; print str; gsub(/cadena/, "secuencia", str); print str }'
```

Es importante notar que $sub()$ y $gsub()$ regresan como valor el número de sustituciones realizadas, como muestra el siguiente ejemplo:
```{bash}
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; print str; print(sub(/cadena/, "secuencia", str)) }'
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; print str; print(gsub(/cadena/, "secuencia", str)) }'
```

Ahora veamos la función $substr()$, que regresa una subcadena de la cadena a modificar:
- $substr()$
```{bash}
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; print str; print(substr(str, 1, 11))}'
awk 'BEGIN { str = "esta es una cadena de caracteres: cadena"; fin = substr(str, 12); print fin}'
```

Noten que $substr()$ regresa la subcadena correspondiente.


## Ejemplos integrativos básicos para entender cómo funciona $AWK$
En esta sección aprenderemos algunos idiomas de $AWK$ muy útiles, que muestran cómo implementar conjuntamente algunas de las variables y elementos de sintaxis listados arriba. 
Con ellos podrán apreciar mejor la lógica, simpleza y utilidad de $AWK$.

### contar longitud de una cadena (oligonucleótido de DNA)
- Fíjate en las diferencias y entiende porqué los dos comandos que muestro seguidamente no dan el mismo resultado
```{bash}
# Fíjate en las diferencias y entiende qué pasa;
echo    atattGAATTCTAGCACATACTAACGGACC | wc -c
echo    atattGAATTCTAGCACATACTAACGGACC | awk 'END{print "oligo",  $0, "tiene", length($0), "nt de longitud"}'
```

- Si no lo has descubierto aún, tal vez te ayude la siguiente salida
```{bash}
# Compara con la salida del bloque anterior
echo    atattGAATTCTAGCACATACTAACGGACC | wc -c
echo -n atattGAATTCTAGCACATACTAACGGACC | wc -c
```

$wc$ cuenta los espacios y saltos de línea como caracteres. $awk$, en modo estándar de *parseo* de registros, ignora los espacios, tabuladores y saltos de línea, ya que definen el **separador de registro** estándar.

### eliminar filas en blanco de un archivo
- Generemos primero un archivo temporal con secuencias en formato FASTA y espacios entre ellas
```{bash}
# genera un archivo temporal con secuencias en formato FASTA y espacios entre ellas
echo -e ">seq1\nATGCATGC\n\n>seq2\nTATTACCAG\n\n>seq3\nATTATTGGC\n\n" > sequences.tmp

# despliega el archivo numerando las líneas
cat -n sequences.tmp
```

- ahora usa $awk$ para imprimir sólo las líneas que contengan al menos 1 campo NO vacío
```{bash}
awk 'NF > 0' sequences.tmp | nl
```

### Imprime sólo ciertas líneas del archivo
- imprime las 6 primeras líneas
```{bash}
awk 'NR <= 6' sequences.tmp | cat -n
```

- imprime las primeras 2 líneas y a partir de la sexta
```{bash}
awk 'NR <= 2 || NR >= 6' sequences.tmp
rm sequences.tmp
```

### filtra la salida de $ls\ -l$ para ver sólo los permisos asociados a archivos y directorios selectos
Es importante que entiendas las diferencias de salida de ambos comandos, es decir, 
el efecto del bloque **END{}** y la diferencia de imprimir **NF** y **$NF**
```{bash}
ls -ld working_with* *.awk *.sh docs pics | awk 'END{print NF}' 
echo '-----------------------------------'
ls -ld working_with* *.awk *.sh docs pics | awk '{print $1, $NF}' 
```

### filtrado de archivos con AWK - procesadores en /proc/cpuinfo
- Cuenta el número de procesadores de tu sistema:
```{bash}
# el archivo /proc/cpuinfo contiene la información sobre las cpus del sistema, incluyendo los cores/procesadores contenidos en la unidad de procesamiento central
head -5 /proc/cpuinfo 
```

Recordemos la sintaxis general de código AWK: <code>/patrón/ { acción }</code>
```{bash}
# usamos el patrón '/^processor/, seguido de la acción {cuenta instancias} y terminamos con un bloque END{} que imprime el valor de la variable n
# este código de AWK se lo pasamos directamente al intérprete de comandos awk como un una cadena entre comillas, seguido del archivo a procesar
# awk 'CODIGO AWK' ARCHIVO_A_PROCESAR
#       PATRON    ACCION END{s}
awk '/^processor/ {n++} END{ print "This computer has", n, "processors"}' /proc/cpuinfo 
```

- Imprime líneas con 12 o menos caracteres de entre las primeras 20 líneas del archivo /proc/cpuinfo
```{bash}
# con head -20 filtramos las primeras 20 líneas, las cuales pasamos a awk con |
# recuerda: la acción por defecto de awk es imprimir, en este caso las líneas que satisfagan la condición
head -20 /proc/cpuinfo | awk 'length <= 12'
```

- Imprime líneas con 30 o más caracteres de entre las primeras 20 líneas del archivo /proc/cpuinfo
```{bash}
head -20 /proc/cpuinfo | awk 'length >= 30'
```

- Imprime líneas que contienen información sobre tamaño del caché
```{bash}
#     PATRON   acción por defecto: imprimir
awk '/cache size/'  /proc/cpuinfo 
```

- Suma el tamaño de cache total del caché de los procesadores
```{bash}
# como ven en la salida anterior, el campo 4 contiene el tamaño de caché de cada procesador
#     PATRON       ACCION    END{}
awk '/cache size/ {size+=$4} END{print "total chache size: ", size, "Kb, ", size/1024, "Mb"}'  /proc/cpuinfo
```

### filtrado de archivos con AWK - suma de tamaños de archivos en Mb

- Suma el tamaño en Mb de los archivos working_with_linux_commands.* en el directorio actual
```{bash}
# Noten que puedo partir líneas de comandos muy largas usando \ antes de introducir un salto de línea ;)
# sumamos el valor del campo #5 de la salida de ls -l a la variable s (s+=$5);
# hacemos algo de aritmética, para convertir los bytes a Mega-bytes (Mb=s/1024^2)
# usamos además printf "formatting-string", var1, var2 ... para imprimir las variables con formato: string = %s, número flotante=%.2f
ls -l working_with_linux_commands.* | \
 awk 's+=$5; END{Mb=s/1024^2; s1="total size of working_with_linux_commands.* files is:"; s2="Mb"; printf "%s %.2f %s\n", s1, Mb, s2}'
```

- comparen y entiendan la diferencia del comando anterior con el del siguiente:
```{bash}
ls -l working_with_linux_commands.* | \
 awk '{s+=$5}; END{Mb=s/1024^2; s1="total size of working_with_linux_commands.* files is:"; s2="Mb"; printf "%s %.2f %s\n", s1, Mb, s2}'
```


### uso de función $++$ para contar ocurrencias de un patrón en una tabla
- Cuenta las entradas para el género *Stenotrophomonas* en la tabla *assembly_summary.txt.gz*
```{bash}
# aquí usamos la expresión regular /\sStenotrophomonas/ para filtrar las líneas desadas, 
#  y contamos las occurrencias con la variable c que autoincrementamos con el operador ++
zcat assembly_summary.txt.gz | \
 awk '/\sStenotrophomonas\s/ {c++} END{printf "%s %i %s\n", "Hay", c, "genomas de Stenotrophomonas"}'
```

### Transforma la salida del comando $uniq\ -c$ en un formato de campos separados por comas (archivo $csv$) con cabecera
Vamos a trabajar con el archivo assembly_summary.txt.gz usando herramientas de filtrado que ya conocemos, pero la
salida de la tubería la vamos a editar con $AWK$ para generar un formato tabular

- veamos primero la salida a modificar con $AWK$
```{bash}
# Primero les muestro la salida que quiero transformar con AWK a formato tabular, 
#  generada por herramientas estándar de filtrado que culminando con un sort | uniq -c 

zcat assembly_summary.txt.gz | grep Stenotrophomonas | cut -f8 | cut -d ' ' -f1,2 | \
sort -d | uniq -c | sort -nrk1
```
  - y ahora filtramos la salida del comando anterior con $AWK$, y ordenamos por número decreciente de genomas
```{bash}
# Con la llamada a BEGIN{print "Genus\tspecies\tcount"; OFS=","} imprimimos en primer lugar la cabecera 
#   e indicamos que queremos que el Output Field Separator sea ',', y no espacios simples
# Luego le indicamos el orden en que queremos imprimir los campos {print $2 "_" $3,$1}
# Noten que uso $2 "_" $3 para que esos dos campos queden unidos por un guion bajo

zcat assembly_summary.txt.gz | grep Stenotrophomonas | cut -f8 | cut -d ' ' -f1,2 | \
sort -d | uniq -c | sort -nrk1 | \
awk 'BEGIN{print "species,count"; OFS=","} {print $2 "_" $3,$1}' 
```

Para acabar de redondear esta sección, veamos unos ejemplos sencillos de uso de $awk$ para trabajar con secuencias en archivos multifasta y así reforzar lo aprendido antes de pasar a secciones más avanzadas.

### Trabajando con archivo FASTA: cuenta el número de secuencias en recA_Bradyrhizobium_vinuesa.fna
```{bash}
awk '/^>/ {s++} END{print FILENAME, "has", s, "sequences"}' recA_Bradyrhizobium_vinuesa.fna
```

- lo mismo, pero con un bucle for (ver sección de programación $Bash$ más abajo) para contar las secuencias en cada archvo *fna del directorio
```{bash}
for f in *fna; do awk '/^>/ {s++} END{print FILENAME, "has", s, "sequences"}' $f; done
```

### Trabajando con archivo FASTA: ¿cuántos nucleótidos hay en recA_Bradyrhizobium_vinuesa.fna?
Para este ejercicio necesitamos eliminar las líneas que contienen las cabeceras FASTA. 

- Noten que usamos la sintaxis '/(\^>|\^$)/' para agrupar dos *regexes* con el fin de saltarnos las líneas que inician con '>' y las que están en blanco.
  Veamos el efecto de la *regex* imprimiendo sólo las primeras 15 líneas:

```{bash}
awk '! /(^>|^$)/' recA_Bradyrhizobium_vinuesa.fna | head -15
```

- podríamos estar tentados de parsarle esta salida a $wc\ -c$ para contar los caracteres
```{bash}
awk '! /(^>|^$)/' recA_Bradyrhizobium_vinuesa.fna | wc -c
```
Pero este resultado no es correcto, ya que $wc\ -c$ contabiliza también los saltos de línea. 

- Esto lo podemos demostrar con el siguiente comando $tr\ --delete\ '\n'$ que elimina los saltos de línea al final de cada línea
```{bash}
awk '! /(^>|^$)/' recA_Bradyrhizobium_vinuesa.fna | tr --delete '\n$' | wc -c
```

- o concatenando directamente en $awk$ las líneas, teniendo cuidado de usar $printf$ indicándole que imprima la cadena resultante SIN salto de línea al final. 
  Noten que '{s=s$1}' concatena cada línea a la variable s, la cual tenemos que imprimir al final, en un bloque END{}, pero usando 'printf "%s", s' para que no imprima el salto de línea
```{bash}
awk '! /(^>|^$)/ {s=s$1} END{printf "%s", s}' recA_Bradyrhizobium_vinuesa.fna | wc -c
```

- si usamos una llamada a $print$, que imprime un salto de línea, nuestra cuenta fallaría por un residuo: contaríamos erróneamente el salto de línea que $print$ introduce al final, como demuestra el siguiente ejemplo:

```{bash}
awk '! /(^>|^$)/ {s=s$1} END{print s}' recA_Bradyrhizobium_vinuesa.fna | wc -c
```

- Con $awk$ obtenemos fácilmente la solución, si tenernos que preocupar de saltos de línea o líneas en blanco, con el siguiente código que hace uso de la función $length()$
```{bash}
awk '! /(^>|^$)/ {l+=length($1)} END{print FILENAME, "has", l, "nts or", l/1000, "kb"}' recA_Bradyrhizobium_vinuesa.fna
```

Notas: 
1. También pudimos haber usado l+=length($0), ya que cada registro (línea en este caso) tiene un solo campo: la cadena de nucleótidos correspondiente. 
2. El código $awk$ del ejemplo de arriba no cuenta los saltos de línea ni líneas en blanco, ya que la función $length()$ sólo contabiliza caracteres 

## **Arreglos asociativos (hashes)** en $awk$ - una estructura de datos muy poderosa y versátil
Los arreglos son un tipo de variable que permite guardar a uno o varios elementos. 
Al arreglo (o $array$), como a cualquier otra variable, la nombramos con un nombre corto pero informativo, y accedemos a los valores almacenados en él mediante un **índice**. 
Éste puede ser numérico o una cadena de caracteres. Es decir, **en $awk$ todos los arreglos son *asociativos*!**. PUedes imaginarlos como una tabla no rígida de **asociaciones llave-valor**.

- Sintaxis de asignación, recuperación y eliminación de valores de un **arreglo** de $awk$
```
# 1. asignación
nombre_arreglo[índice] = value

# 2. recuperación de un valor particular
print nombre_arreglo[índice]

# 3. comprobar la existencia de un índice o llave en un arreglo
if (índice in nombre_arreglo) {haz algo con índice}

también puede usarse una notación más corta:
(índice in nombre_arreglo) {haz algo con índice}

# 4. extraer todos los valores del arreglo
for (i in nombre_arreglo) print "índice ", i , " guarda valor ", nombre_arreglo[índice]
</code>

# 5. eliminar un elemento del arreglo (el par llave-valor)
delete arreglo[índice]

# 6. vaciar un arreglo completamente
delete arreglo

# 7. llenar un arreglo con la función split()
split(cadena, arreglo [,separador_de_campo [, seps]])
```

En la mayoría de los lenguajes los arreglos tienen índices numéricos, los cuales definen la posición en el arreglo en el que se guardó el elemento correspondiente. Es decir, en ese caso se trata de listar ordenadas posicionalmente en el orden en el que los elementos fueron agregados a la lista.

Los **arreglos asociativos** o **hashes** establecen una *asociación* entre los $índices$ y los $elementos$ del arreglo. Es decir, para cada elemento del arreglo se mantienen un par de valores: el índice el elemento y su valor. Es importante notar que bajo este esquema, los elementos del arreglo no se guardan en ningún orden predeterminado, es decir, los **arreglos asociativos** son *arreglos desordenados*. Por ello, si bien podemos usar índices numéricos, éstos no definen un orden en una lista ordenada. Son simples etiquetas. No obstante, si se asignan índices numéricos consecutivos a los valores que se guardan en un arreglo, se pueden recuperar en dicho orden al correr un bucle iterativo. 

Veamos unos ejemplos de estos conceptos y del uso de las funciones de manipulación de arreglos en $awk$

### Uso de la funciones básicas de manipulación de $arreglos$ en $awk$: **split()**, **delete()** ...

La función $split()$ divide una *cadena* en unidades separadas por el *separador de campo*, guardándolas en *arreglo* y las cadenas separadoras en el arreglo *seps*, acorde a la siguiente sintaxis:

- <code>split(cadena, arreglo [,separador_de_campo [, seps]])</code>

Veamos un ejemplo que implementa $split()$ y otras funciones básicas de manipulación de *arreglos*, como asignaciones, eliminación de valores e impresión de todos los pares llave-valor guardados en un arreglo.

```{bash}
# 1. llenado del hash features con nombres de los campos de la tabla assembly_summary.txt.gz,
#     que quedarán indexados por números consecutivos, que corresponden a su número de columna
#     usando la función split(), aplicada al segundo registro de la tabla (la cabecerea) 
#      features[1] = "assembly_accession"; features[2] = "bioproject"; ...
zcat assembly_summary.txt.gz | awk 'NR==2 { split( $0, features, "\t", seps ) } END { print features[8]; delete features[8]; print "ahora features[8] está vacío: [",features[8],"]\n\n"; for (f in features) print f, features[f];  features[8] = "taxon"; print "\nahora features[8] fue reasignado: ", features[8]}'
```

### Modelado de archivos FASTA (mini_fasta.fst) con $hashes$ de $awk$
- Primero vamos a desplegar el archivo FASTA
```{bash}
# despliega el archivo mini_fasta.fst
cat mini_fasta.fst
```
- El siguiente código lee el archivo mini_fasta.fst en un $hash$ o $arreglo\ asociativo$ llamado *seqs* e imprime un archivo en formato "FASTAB"
```{bash}
# Si la línea inicia con un '>', inicializamos la variable s="" y guardamos $0 en h (header)
#  después de leer la línea de cabecera (header), sabemos que lo que sigue son líneas de secuencia, hasta el siguiente '>'
#  Por ello concatenamos estas líneas y asignamos la cadena resultante a la variable s. 
#  Cuando awk encuentra la siguiente línea que empieza con '>', inicializa s="" y captura la nueva cabecera en h
#  Finalmente, después de haber leído el archivo completo, abrimos un bloque END{}, desde el cual corremos un
#   blucle for para iterar sobre las llaves o índices del hash, que guardamos en h, 
#   imprimiendo h y el valor asociado con h, "\t", seqs[h], generando lo que yo llamo un archivo "FASTAB"
 awk 'BEGIN{OFS="\t"} {if(/^>/){s=""; h=$1}else{s=s$1}; seqs[h]=s;} END{for (h in seqs) print h, seqs[h]} ' mini_fasta.fst >  mini_fasta.fastab
 cat mini_fasta.fastab
```
- La que sigue es una variante del código de arriba, pero modelando la estructura de registros FASTA con RS=">"; FS="\n" y usando bucle for (var, condición, autoincremento_var)
```{bash}
#    BEGIN{ INICIALIZACIÓN }         PATRON                 { PROGRAMA }                           END { BLOQUE FINAL PARA IMPRIMIR RESULTADOS}
awk 'BEGIN{RS=">"; FS="\n"; OFS="\t"} NR > 1 {s=""; h=$1; for (i=2; i<=NF; i++) s=s$i; seqs[h]=s;} END{for (h in seqs) print h, seqs[h]} ' mini_fasta.fst
```

Noten que el **archivo FASTAB** resultante lo podemos filtrar por ejemplo con $grep$, si queremos sacar líneas específicas.

```{bash}
# Filtrar con grep el archivo FASTAB para sacar las secuencias 1,3 y 5
grep -E 'seq1|seq3|seq5'  mini_fasta.fastab > selected_seqs.fastab
```

- El siguiente código convierte un archivo en formato "FASTAB" a un FASTA canónico
```{bash}
# como el archivo que se lee es un "FASTAB", inicializamos el FS="\t"
# dado que queremos imprimir los campos header y secuencia en líneas diferentes, inicializamos OFS="\n"
awk 'BEGIN{FS="\t"; OFS="\n"} {print $1,$2}'  selected_seqs.fastab > selected_seqs.fas
cat selected_seqs.fas

rm selected_seqs.fa*
```

### Juntar dos tablas que comparten un campo común (**llave primaria**) usando un $hash$
Como hemos venido discutiendo, en el campo de la bioinformática y de bases de datos en general, las relaciones entre las variables asociadas a un registro se modelan frecuentemente usando tablas. Siguiendo un principio básico del modelo relacional, se usan distintas tablas para guardar variables relacionadas entre ellas. Para evitar excesiva redundancia entre las tablas, éstas contienen típicamente una llave común o **llave primaria** que las asocia inequívocamente con un registro particular. Columnas particulares de las diferentes tablas pueden relacionarse a través de la **llave primaria**. Esto se modela fácilmente con $hashes$ indexándolos con las **llaves primarias** de las tablas. Cabe mencionar que $gawk$ maneja también arreglos multidimensionales, por lo que se podrían usar para indexarlos con **llaves primarias y secundarias**, pero no veremos aquí estos casos más complejos.

Veamos el siguiente ejemplo, en el que juntamos dos tablas con información no redundante sobre ensambles de genomas, pero que comparten el número de accesión de los mismos como **llave primaria**. La primera tabla contiene la información taxonómica y la segunda información relativa al ensamble en sí.

```{bash}
echo ">>> mini_tabla_parte1.tsv <<<"
cat mini_tabla_parte1.tsv
echo
echo ">>> mini_tabla_parte2.tsv <<<"
cat mini_tabla_parte2.tsv
```

En el código que se muestra abajo hacemos lo siguiente:
1. inicializamos variables FS y OFS a "\t" para leer e imprimir campos separados por tabuladores
2. cuando leemos el primer archivo, es decir, cuando NR==FNR, llenamos el arreglo a, indexado por el campo #1 del primer archivo (assembly_accession) y asignando como valor la línea completa
3. el bloque de ACCION-2 se ejecuta al leer tabla2; si su campo #1, que contiene la llave primaria (assembly_accession), existe en el arreglo a, se ejecuta la acción de imprimir el valor de a[$1] (de la tabla1) 
    seguido de los campos $2 a $4 de la tabla 2, evitando imprimir el campo común entre ambas tablas $1
```{bash}
#       INICIALIZACION      PATRON    ACCION-1         CONDICION-1         ACCION-2                 tabla1                tabla2
awk 'BEGIN { FS=OFS="\t" } NR==FNR { a[$1]=$0; next } { if ($1 in a) { print a[$1], $2, $3, $4 } }' mini_tabla_parte1.tsv mini_tabla_parte2.tsv

# la que sigue es una notación equivalente, algo más corta
# awk 'BEGIN { FS=OFS="\t" } NR==FNR { a[$1]=$0; next } ($1 in a) { print a[$1], $2, $3, $4 }' mini_tabla_parte1.tsv mini_tabla_parte2.tsv
```

### $hashes$ y conteo de instancias de cadenas específicas usadas como llaves
Sigue otro ejemplo de la versátil estructura de datos conocida como $arreglo\ asociativo$ o $hash$, la cual es muy útil para contar las instancias de ocurrencia de cadenas específicas al usarlas como **llave del** $hash$ y usando el **operador ++**, como muestra el siguiente código genérico:

-<code>h[llave]++</code>

Veamos uno ejemplo:

- Cuenta el número de genomas en cada nivel de ensamble (campo #12 *assembly_level*) para un taxon particular, pasado al programa como argumento desde la línea de comando
  - Noten que tienen que pasar el TAXON a la variable tax, con la siguiente sintaxis <code> awk 'programa' tax=TAXON</code>
  - usamos el $hash$ *al* para contar las instancias de la variable categórica *assembly_level* con el código <code>al[$12]++</code>
```{bash}
zcat assembly_summary.txt.gz | awk 'BEGIN{ FS=OFS="\t"; print "taxon\tassembly_level\tn_genomes" } $8 ~ tax { al[$12]++ } END{ for(l in al) if (l != "") print tax, l, al[l] }' tax=Pseudomonas
```

### Ordenamiento de la salida de un $hash$ de $gawk$ por *llave* o *valor* manipulando la variable de ambiente **PROCINFO["sorted_in"]**
Noten que el resultado anterior no sale ordenado ni por la llave ni por el valor. Como ya mencionamos, internamente los $hashes$ de $gawk$ se procesan arbitrariamente por ser **arreglos asociativos**.

Pero en $gawk$ podemos manejar la *variable de ambiente* **PROCINFO["sorted_in"]** para pasarle instrucciones predeterminadas de ordenar la salida como **"@ind_str_asc"**, **"@ind_num_asc"**, **"@val_str_asc"** o **"@val_num_asc"**, entre otros, como podrás consultar en el [$gawk$ manual](https://www.gnu.org/software/gawk/manual/gawk.html)

Para ello tenemos que asignarle estas funciones internas de ordenamiento a $PROCINFO["sorted\_in"]$ dentro de un bloque $BEGIN{}$, como muestran los ejemplos que siguen abajo.

1. ordena la salida por llave con **PROCINFO["sorted_in"] = "@ind_str_asc"**
```{bash}
zcat assembly_summary.txt.gz | awk 'BEGIN{FS=OFS="\t"; print "taxon\tassembly_level\tn_genomes"; PROCINFO["sorted_in"] = "@ind_str_asc"} $8 ~ tax {al[$12]++} END{ for(l in al) if (l != "") print tax, l, al[l] }' tax=Pseudomonas
```
2. ordena la salida por valor con **PROCINFO["sorted_in"] = "@val_num_asc"**
```{bash}
zcat assembly_summary.txt.gz | awk 'BEGIN{FS=OFS="\t"; print "taxon\tassembly_level\tn_genomes"; PROCINFO["sorted_in"] = "@val_num_asc"} $8 ~ tax {al[$12]++} END{ for(l in al) if (l != "") print tax, l, al[l] }' tax=Pseudomonas
```

Cabe señalar que según las características de la llave, ésta puede que se imprima de manera ordenada, como muestra el siguiente ejemplo:

- Cuenta el número de genomas liberados por año para un taxon indicado en la línea de comando
  - Noten que tienen que asignar el 'TAXON' a la variable $tax$, con la siguiente sintaxis <code> awk 'programa' tax=TAXON</code>
  - Además hacemos uso de la **función interna gsub()** que permite hacer sustituciones globales con la sintaxis: <code>gsub(/regex/, reemplazo)</code>
```{bash}
zcat assembly_summary.txt.gz | awk 'BEGIN{FS="\t"; print "taxon\trel_y\tn_genomes"} $8 ~ tax {gsub(/\/.*$/, ""); count_y[$15]++} END{for (y in count_y) if (y > 0) printf  "%s\t%d\t%d\n", tax, y, count_y[y]}' tax=Pseudomonas
```

## **$awk$ scripts**
Cuando el programa que escribimos es un poco largo y sea de interés general, es decir, que se vaya a usar regularmente, conviene guardarlo en un archivo. En este bloque les presentaré un primer $script$ que integra múltiples aspectos presenados anteriormente y varios elementos nuevos del lenguaje. Pero más importante aún, el $script$ *count_genome_features_for_taxon.awk* es un primer ejemplo de los aspectos que como programadores debemos cuidar para que el código sea robusto y amigable con el usuario, para facilitarle su uso. 

Para que un programa sea útil, debe resolver adecuadamente una clase o tipo de problemas o acciones que se realicen rutinariamente. En secciones anteriores hemos mostrado múltiples ejemplos de uso de $awk$ para parsear la tabla assembly_summary.txt.gz. Dado que esta tabla provee información clave sobre los ensambles disponibles en la división RefSeq de GenBank, podría ser útil tener un $script$ que nos provea de estadísticas de resumen (conteos) para campos específicos de la misma y para un taxon de nuestro interés particular. 

En la sección sobre [uso del operador ++ para contar ocurrencias de un patrón](#uso-de-función-para-contar-ocurrencias-de-un-patrón-en-una-tabla) vimos dos ejemplos muy parecidos, uno para contar el número de ensambles liberados por año para un taxon y otro para contar el nivel del ensamble. Estos programas anteriores se parecen mucho y no sería una buena práctica guardarlos como $scripts$ individuales. Más bien debemos pensar en cómo generalizarlos para que puedan calcular estas estadísticas de resumen sobre cualquier campo relevante de la tabla. El $script$ lo llamaremos *count_genome_features_for_taxon.awk* y haremos su uso flexible y práctico al pasarle dos argumentos desde la línea de comandos: *<tax=TAXON> <numero_de_columna>*.

Este $script$ *count_genome_features_for_taxon.awk* implementa una gama de elementos sintácticos, como funciones integradas y variables reservadas, la mayoría de las cuales hemos descrito en secciones previas. 
Las siguientes subsecciones explican brevemente los que no hemos visto en detalle aún, como estructuras de control.

### Argumentos y arreglos ARGC y ARGV 
$awk$ define automáticamente las variables **ARGC** y **ARGV** para proveer de información al programa que las usa sobre los argumentos pasados al mismo.

Los argumentos pasados al programa desde la línea de comandos se guardan en $ARGV$. 
Al contrario que la mayoría de los *arreglos* de $awk$, ARGV se indexa desde 0 .. ARGC - 1, como se muestra en el siguiente ejemplo:

```{bash}
awk 'BEGIN { print "# ARGV contiene:"; for (i=0; i < ARGC; i++) print ARGV[i]; print "\nARGC =", ARGC}' archivo1 archivo2
```

Como ven, ARGV[0] contiene a $awk$ y ARGC es igual al número de argumentos recibidos + 1

Noten que el programa en sí no es guardado en ARGV. Tampoco se guardan en ARGV/ARGC las opciones y argumentos que se le asignan al programa antes de definir el bloque de código, 
como muestra el siguiente ejemplo que llama al programa *showargs.awk*

```{bash}
cat showargs.awk
```

```{bash}
awk -v A=1 -f ./showargs.awk B=2 mini_fasta.fst
```

### Condicionales: if; if-else if; if-else if-else
Los *condicionales* son estructuras de control fundamentales para controlar el flujo de un programa en función de si son ciertas o falsas determinadas condiciones. 
Sólo si una condición es cierta, se ejecuta la acción subsiguiente, según la siguiente sintaxis:

- <code>if(condición){acción}</code>

Se pueden evaluar varias condiciones secuencialmente con la siguiente sintaxis:

- <code>if(condición){acción}else if(condición){acción}else if(condición){acción} ...</code>

De no cumplirse ninguna de ellas, podemos indicar una acción a realizarse en dicho caso con la siguiente sintaxis:

- <code>if(condición){acción}else if(condición){acción}else if(condición){acción}else{acción}</code>

Veamos un ejemplo sencillo:

```{bash}
awk 'BEGIN { n = 3; arg=ARGV[1]; gsub(/n=/, "", arg);  if (n == arg) { print n, "==", arg } else { print n, "!=", arg }} ' n=2
awk 'BEGIN { n = 3; arg=ARGV[1]; gsub(/n=/, "", arg);  if (n == arg) { print n, "==", arg } else { print n, "!=", arg }} ' n=3
```

### El $script$ *count_genome_features_for_taxon.awk*
El $script$ *count_genome_features_for_taxon.awk* tiene por finalidad mostrarles un ejemplo de código un poco más complejo y extenso con el fin de:

1. integrar y revisar los diferentes aspectos sintácticos del lenguaje discutidos hasta ahora
2. mostrar cómo llamar un $script$ y pasarle argumentos desde la línea de comandos usando la siguiente sintaxis:

  - <code>awk -f script_name arg=x arg2=y</code> 
  
Veamos el $script$ completo y estudiemos sus secciones:
```{bash}
cat ./count_genome_features_for_taxon.awk
```

Ahora veremos diferentes llamadas al $script$. Debes revisar la salida de cada una y cotejarla con el código, asegurándote que entiendes porqué se genera cada una.

- llamada al $script$ *count_genome_features_for_taxon.awk* con < 2 argumentos imprime una ayuda
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk
```
- el $script$ sólo se ejecuta si le pasamos los índices (números de columnas) adecuados de la tabla assembly_summary.txt.gz
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk tax=Pseudomonas f=1
```
- el $script$ sólo revisa que el usuario use la sintaxis requerida para pasar argumentos desde la línea de comandos
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk Pseudomonas 5
```
- el campo #5 imprime conteos de *refseq_category*
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk tax=Pseudomonas f=5
```
- el campo #12 imprime conteos de *assembly_level*
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk tax=Pseudomonas f=12
```
- el campo #15 imprime conteos de *seq_rel_date*
```{bash}
zcat assembly_summary.txt.gz | awk -f ./count_genome_features_for_taxon.awk tax=Pseudomonas f=15
```

### Shebang line #!/usr/bin/awk y scripts autocontenidos: el $script$ **filter_fasta_sequences.awk**
Para finalizar veamos un $script$ muy sencillo pero sin duda últil escrito en $AWK$ para filtrar secuencias de un archivo multifasta, 
pasándole una cadena de cadena de caracteres que el $script$ usará para filtrar el archivo, imprimiendo sólo aquellas secuencias que
coinciden con la cadena de filtrado.

Veamos el $script$ [filter_fasta_sequences.awk](https://github.com/vinuesa/intro2linux/blob/master/filter_fasta_sequences.awk), 
disponible en el repositorio [GitHub - intro2linux](https://github.com/vinuesa/intro2linux).

- sus permisos
```{bash}
[ -s filter_fasta_sequences.awk ] && ls -l filter_fasta_sequences.awk
```

- el código
```{bash}
[ -s filter_fasta_sequences.awk ] && cat filter_fasta_sequences.awk
```

- Llamada al $script$ sin argumentos imprime su ayuda
```{bash}
# como el directorio donde vive el script no está en el path, a pesar de ser ejecutable, 
#  hay que llamarlo indicando el path absoluto o relativo al mismo
./filter_fasta_sequences.awk

```

- Llamada al $script$ con los **argumentos posicionales** <cadena_de_filtrado> <nombre_archivo_fasta_a_filtrar>
```{bash}
# como el directorio donde vive el script no está en el path, a pesar de ser ejecutable, 
#  hay que llamarlo indicando el path absoluto o relativo al mismo
# Le pasamos el nombre de la cepa BES-1 que está en el archivo recA_Bradyrhizobium_vinuesa.fna
./filter_fasta_sequences.awk BES-1 recA_Bradyrhizobium_vinuesa.fna
```

- Filtra las secuencias sin clasificar (genosp)
```{bash}
# como el directorio donde vive el script no está en el path, a pesar de ser ejecutable, 
#  hay que llamarlo indicando el path absoluto o relativo al mismo
# Le pasamos el nombre de la cepa BES-1 que está en el archivo recA_Bradyrhizobium_vinuesa.fna
# NOTA: filtro la salida con grep '^>' para ver sólo las cabeceras FASTA y evitar quesea demasiado copiosa
#       quítale el grep final, si quieres ver el archivo con sus secuencias
./filter_fasta_sequences.awk genosp recA_Bradyrhizobium_vinuesa.fna | grep '^>'
```

- uso de *filter_fasta_sequences.awk* dentro de una tubería de comandos

```{bash}
# el script se puede llamar también desde dentro de una tubería, es decir,
# pasándole el STDOUT de un comando previo entubado con | al STDIN del script
# NOTA: filtro la salida con grep '^>' para ver sólo las cabeceras FASTA y evitar quesea demasiado copiosa
#       quítale el grep final, si quieres ver el archivo con sus secuencias
cat recA_Bradyrhizobium_vinuesa.fna | ./filter_fasta_sequences.awk MAM  | grep '^>'
```

### Filtra un archivo multifasta usando un $hash$ y una lista de etiquetas con *get_sequences_from_list.awk*
Este problema es una variante del anterior. En este caso queremos filtrar el multifasta usando un archivo que contiene la lista de etiquetas de nuestro interés.

Para ello vamos a usar el script *get_sequences_from_list.awk*, que recibe dos argumentos:

<code>get_sequences_from_list.awk <lista_de_etiquetas> <archivo_fasta></code>

El archivo de etiquetas debe tener el siguiente formato:
<pre>
>etiqueta1
>etiqueta2
> ...
</pre>

Veamos las etiquetas (archivo *seq.list*) que queremos extraer del archivo *mini_fasta.fst*

```{bash}
cat seq.list
```

Estudiemos ahora el $script$ *get_sequences_from_list.awk*

```{bash}
cat get_sequences_from_list.awk
```

Corramos el $script$ *get_sequences_from_list.awk*, pasándole los dos nombres de archivo como argumentos, primero la lista, luego el FASTA a filtrar

```{bash}
awk -f get_sequences_from_list.awk seq.list mini_fasta.fst
```

Noten que ¡el orden de las secuencias en la salida del $script$ *get_sequences_from_list.awk* no coincide con el de la lista! Ello se debe a que, como indicamos en la sección de introducción de $arreglos$, los $hashes$ o $arreglos\ asociativos$ no guardan los índices o llaves en un orden particular. 

### Ordenar las secuencias de un archivo en formato **FASTAB** según las entradas de una lista en un bucle $while$
Si necesitan ordenar las secuencias siguiendo un orden particular, indicado en una lista, una opción es filtrar un archivo **FASTAB** con un bucle $while$ de $Bash$ como se muestra a continuación. Veremos la sintaxis de *bucles while* de $Bash$ en una sección posterior.

```{bash}
while read seq; do grep "$seq" mini_fasta.fastab; done < seq.list
```

Ordenar las etiquetas acorde a una lista puede ser útil por ejemplo si queremos recuperar las secuencias de un clado filogenético en particular, para el cual tenemos la lista de sus miembros.

### Uso de $hashes$ para concatenar secuencias en dos o más archivos FASTA
El siguiente código concatena las secuencias en dos archivos FASTA. Cada uno contiene la secuencia de un gen distinto, del mismo conjunto de organismos, identificados con las mismas etiquetas en las cabeceras FASTA de los dos archivos.

```{bash}
echo ">>> mini_fasta.fst <<< "
cat mini_fasta.fst
echo
echo ">>> mini_fasta2.fst <<< "
cat mini_fasta2.fst
```

- Aquí el código para concatenar las secuencias de los dos archivos que comparten la misma etiqueta o cabecera FASTA usada como llave del $hash$
```{bash} 
#     INICIALIZACION                     PATRON-1                               ACCION-1                        PATRON-2                        ACCION-1                               END{ IMPRIME¨}
awk 'BEGIN{RS=">"; FS="\n"; OFS="\n"} NR > 1 && NR <= FNR {s=""; h=$1; for (i=2; i<=NF; i++) s=s$i; seqs[h]=s;} NR > FNR {s=""; h=$1; for (i=2; i<=NF; i++) s=s$i; seqs[h]=seqs[h]s; } END{ for (h in seqs) if(h){ print ">"h, seqs[h]}} ' mini_fasta.fst mini_fasta2.fst
```

### Retos de programación con $hashes$
1. En base al ejemplo anterior, transforma en $scripts$ autocontenidos a los *oneliners* mostrados más arriba que convierten un archivo fasta a formato FASTAB (nómbralo *fas2tab.awk*) 
y a su contraparte, el que lo regresa a formato FASTA, nómbralo tab2fas.awk
2. Modifica el script *count_genome_releases_for_taxon_by_year.awk* para que cuente el número de genomas para un taxon particular que hay en cada nivel de ensamble (campo #12 $assembly\_level$) y llámalo *count_genome_assemly_levels_for_taxon.awk* 
3. Modifica el *one-liner* que concatena múltiples archivos FASTA para que corra como un $script$ autocontenido, que puedas invocar como *concat_fastas.awk*

## Tutoriales recomendados para profundizar en AWK
Aprender $AWK$ es muy valioso y redituable. Es un lenguaje compacto que puedes aprender en un fin de semana dedicado a ello. No sólo te convertirá rápidamente en un usuario avanzado del $Shell$, sino que te permitirá apreciar y aprovechar mucho mejor su poder, versatilidad y belleza.

- [tutorial de unos 20 min](https://ferd.ca/awk-in-20-minutes.html)
- [tutorial extenso, excelente](https://www.grymoire.com/UNIX/Awk.html)
- [gawk manual, la guía definitiva](https://www.gnu.org/software/gawk/manual/)

Veremos más ejemplos de $gawk$ en la sección de ejercicios integrativos que sigue a continuación.

******

# Ejercicios integrativos de uso de herramientas de filtrado del $Shell$
En esta sección retomaremos el $Shell$ para filtrar archivos, integrando $awk$ cuando sea pertinente.

## Filtrado de archivos separados por tabuladores (tablas) con AWK y su graficado con R
- Asocia cada nombre de columna de la tabla assembly_summary.txt.gz con su número de campo
```{bash}
# asocia cada nombre de columna de la cabecera con el número de la columna correspondiente
zcat assembly_summary.txt.gz | head -2 | sed '1d; s/\t/\n/g' | cat -n
```

- cuenta aquellas entradas de la tabla que tienen un número de accesión revisado v2
Esta información la encontramos en el campo #16 de la tabla, llamado *asm_name*, como podemos ver en la salida anterior. Exploremos dicho campo
  - veamos las primeras 10 líneas de la columna #16 (campo asm_name)
```{bash}
# veamos las primeras 10 líneas de la columna #16 (campo asm_name)
zcat assembly_summary.txt.gz | cut -f 16 | head
```
  - esta línea nos permite visualizar algunas variaciones de cadenas de caracteres del campo 16 que terminan en 2
```{bash}
# Veamos posibles variaciones de cadenas de caracteres del campo 16 que terminan en 2
zcat assembly_summary.txt.gz | cut -f 16 | grep '2$' | head
```
  - ahora que conocemos la estructura y variación de las cadenas de caracteres del campo 12, usemos $awk$ para hacer el trabajo
```{bash}
# >>> ojo, es importante definer FS="\t", para que tome como campos sólo a aquellos separados por tabulador, y no considere los espacios <<<
# ejemplo de código AWK con la estructura: BEGIN_BLOCK, condición, acción, END_BLOCK
# recuerden, como assembly_summary.txt.gz está comprimido, necesitamos zcat para poderlo leer y enviar su STDOUT a awk mediante el pipe '|'
# Noten el uso de la regex /[vV]2$/, que machea cadenas de caracteres que terminan en v2 o V2
#   y el uso de printf para hacer redondeo a dos decimales del porcentaje
zcat assembly_summary.txt.gz | awk 'BEGIN{FS="\t"} $16 ~ /[vV]2$/ {n++} END{perc=n*100/NR; printf "%d %.2f %s\n", n, perc, "%"}'
```

- cuenta aquellas entradas de la tabla que tienen un número de accesión revisado v2, publicados en 2019 para genomas en estado Scaffold
```{bash}
# ejempo de código AWK con la estructura: BEGIN_BLOCK, condición1 && condición2 && condición3, acción, END_BLOCK
zcat assembly_summary.txt.gz | awk 'BEGIN{FS="\t"} $16 ~ /v2$/ && $15 ~ /2019/ && $12 == "Scaffold" {n++} END{print n}' 
```

- veamos las entradas de la tabla que tienen un número de accesión revisado v2, publicados en 2019 para genomas en estado Scaffold, pero imprime sólo los campos organism_name y ftp_path en formato tabla (OFS="\\t"), imprimiendo sólo las primeras 3 líneas
```{bash}
# ejempo de código AWK con la estructura: BEGIN_BLOCK, condición1 && condición2 && condición3, acción
zcat assembly_summary.txt.gz | awk 'BEGIN{FS="\t"; OFS=FS} $16 ~ /v2$/ && $15 ~ /2019/ && $12 == "Scaffold" {print $8, $20}' | head -3
```

- genera una estadística del número de genomas por especie (columna # 8) del género Pseudomonas en formato tabular [EspecieTABnum_genomas], y muestra sólo las especies con al menos 20 genomas secuenciados. Añade una cabecera a la salida.
```{bash}
# genera una estadística del número de genomas por especie (columna # 8) del género Pseudomonas en formato tabular [Especie\tnum_genomas], y muestra sólo las especies con al menos 20 genomas secuenciados!
# Noten que podemos imprimir la cabecera de la tabla desde el bloque BEGIN{}
zcat assembly_summary.txt.gz | grep Pseudomonas | cut -f8 | sort | uniq -c | sort -nrk1 | sed 's/Pseudomonas[[:space:]]/Pseudomonas_/' | awk 'BEGIN{OFS="\t"; print "Especie\tnum_genomas"}{if($1 >= 20) print $2, $1}'
```

- Repitamos el ejercicio anterior, generando un archivo con campos separados por comas (csv), que se puede leer en R para generar un $data\ frame$ de $R$ y generar una gráfica fácilmente. Para ello lo guardamos en un archivo
```{bash}
# Noten que primero imprimimos una cabecera al archivo  Pseudomonas_species_with_gt_20_genomes.csv, y seguidamente le concatenamos con '>>' el resultado del parseo del archivo assembly_summary.txt.gz con nuestra tubería de comandos
echo -e "Especie,num_genomas" > Pseudomonas_species_with_gt_20_genomes.csv
zcat assembly_summary.txt.gz | grep Pseudomonas | cut -f8 | sort | uniq -c | sort -nrk1 | sed 's/Pseudomonas[[:space:]]/Pseudomonas_/' | awk 'BEGIN{OFS=","}{if($1 >= 20) print $2, $1}' >> Pseudomonas_species_with_gt_20_genomes.csv
```

- Visualiza la cabecera del archivo que acabamos de escribir

```{bash}
head -5 Pseudomonas_species_with_gt_20_genomes.csv
```

- Llamemos a R para hacer una gráfica

```
$ R
```

```{r}
# 1. leemos el archivo a una estructura de datos tipo dataframe
dfr <- read.csv(file = "Pseudomonas_species_with_gt_20_genomes.csv", header = TRUE)

str(dfr)

head(dfr)

dotchart(dfr$num_genomas, labels = dfr$Especie, main = "Número de genomas por especie")

```

### Reto de programación $awk$ y $R$

Repite el ejercicio anterior, incluyendo el graficado del número de genomas por especie para el género <i>Acinetobacter</i>, pero graficando sólo aquellas especies con mínimo 5 y máximo 100 genomas

<b>¡Felicidades, ya estás aprendiendo a programar! No es tan difícil, ¿verdad?</b>

## Ejercicios de exploración y parseo de archivos FASTA

Te propongo el siguiente ejercicio con un archivo de secuencias de DNA en formato FASTA para practicar algunos aspectos de lo aprendido en esta primera sesión.

Para correr los ejercicios, asegúrate de tener el archivo recA_Bradyrhizobium_vinuesa.fna en el directorio actual de trabajo.

El archivo recA_Bradyrhizobium_vinuesa.fna contiene secuencias del gen *recA* de bacterias
del género *Bradyrhizobium* depositadas en GenBank por P. Vinuesa. 

### Búsqueda y descarga de secuencias en GenBank usando el sistema ENTREZ
Este bloque muestra el comando que usé para descargarlas usando el sistema ENTREZ de NCBI. 
El comando debe pegarse en la ventana superior del sistema [ENTREZ](https://www.ncbi.nlm.nih.gov/nucleotide?cmd=search).

```
# pega esta sentencia en la ventana de captura para interrogar la base de datos 
# de nucleótidos de NCBI mediante el sistema ENTREZ
'Bradyrhizobium[orgn] AND vinuesa[auth] AND recA[gene]'
```
<img src="fetch_recA_bradys_vinuesa_nuccore_screenshot.png">


No hace falta que las descargues de NCBI, el archivo [*recA_Bradyrhizobium_vinuesa.fna*](https://github.com/vinuesa/intro2linux/tree/master/data) está disponible en el repositorio GitHub, bajo el directorio data

### Inspección y estadísticas básicas de las secuencias descargadas
1. ¿Cuántas secuencias hay en el archivo recA_Bradyrhizobium_vinuesa.fna?
2. Explora la cabecera y cola del archivo con head y tail
3. Despliega las 5 primeras lineas de cabeceras fasta usando **grep** y **head** para explorar su estructura en detalle
4. Calcula el número de generos que contiene el archivo FASTA
5. Calcula el número de especies que contiene el archivo FASTA
6. Imprime una lista ordenada de mayor a menor, del numero de especies que contiene el archivo FASTA

### Edición de las cabeceras FASTA mediante herramientas de filtrado de UNIX
1. Explora nuevamente todas las cabeceras FASTA del archivo recA_Bradyrhizobium_vinuesa.fna usando **grep** y less
2. Simplifica las cabeceras FASTA usando el comando **sed** (stream editor)

El objetivo es eliminar redundancia y los campos gb|no.de.acceso, así como todos los caracteres 
'( , ; : )' que impedirían el despliegue de un árbol filogenético, al tratarse de caracteres
reservados del formato NEWICK.
Dejar solo el numero GI, así como el género, especie y cepa indicados entre corchetes.

Es decir vamos a:
 - reducir Bradyrhizobium a 'B'
 - eliminar ' recombinase ...' y reemplazarlo por ']'
 - eliminar 'genosp. '
 - sustituir espacios por guiones bajos

Nota: hagan uso de expresiones regulares como '.*' y '[[:space:]]'

3. Cuando estén satisfechos con el resultado, guarden la salida del comando en un archivo llamado recA_Bradyrhizobium_vinuesa.fnaed

***

## Solución a la práctica y un ejercicio adicional

Este ejercicio está basado en un capítulo que escribí para el manual de [Sistemática Molecular y Bioinformática. Guía práctica](https://tienda.fciencias.unam.mx/es/inicio/279-sistematica-molecular-y-bioinformatica-guia-practica.html)

1. ¿Cuántas secuencias hay en el archivo recA_Bradyrhizobium_vinuesa.fna?
```{bash}
 grep '^>' recA_Bradyrhizobium_vinuesa.fna | head -5
```

2. Explora la cabecera y cola del archivo con head y tail
```{bash}
 grep '^>' recA_Bradyrhizobium_vinuesa.fna | head -5
```

3. Cuenta el numero de generos y especies que contiene el archivo FASTA
```{bash}
grep '^>' recA_Bradyrhizobium_vinuesa.fna | cut -d' ' -f2,3 | sort | uniq -c
```

4. Imprime una lista ordenada de mayor a menor, del numero de especies que contiene el archivo FASTA
```{bash}
grep '^>' recA_Bradyrhizobium_vinuesa.fna | cut -d' ' -f2,3 | sort | uniq -c | sort -nrk1
```
  
### Edición de las cabeceras FASTA mediante herramientas de filtrado de UNIX

5. Exploremos todas las cabeceras FASTA del archivo recA_Bradyrhizobium_vinuesa.fna usando **grep**
```{bash}
# grep '^>' recA_Bradyrhizobium_vinuesa.fna | less # para verlas por página 
grep '^>' recA_Bradyrhizobium_vinuesa.fna  | head # para no hacer muy extensa la salida
```


6. simplifiquemos las cabeceras FASTA usando el comando **sed** (stream editor)

El objetivo es eliminar redundancia y los campos gb|no.de.acceso, así como todos los caracteres 
'( , ; : )' que impedirían el despliegue de un árbol filogenético, al tratarse de caracteres
reservados del formato NEWICK.
Dejar solo el numero de accesión, así como el género, especie y cepa indicados entre corchetes.

Es decir vamos a:
 - reducir Bradyrhizobium a 'B. '
 - eliminar ' recombination ...' y reemplazarlo por ']'
 - eliminar 'genosp. '
 - sustituir espacios por guiones bajos

Noten el uso de expresiones regulares como '.*' y '[[:space:]]'

```{bash}
sed 's/ Bra/ [Bra/; s/|gb.*| /|/; s/Bradyrhizobium /B_/; s/genosp\. //; s/ recomb.*/]/; s/[[:space:]]/_/g; s/_/ /' recA_Bradyrhizobium_vinuesa.fna | grep '>' | head -5
sed 's/ Bra/ [Bra/; s/|gb.*| /|/; s/Bradyrhizobium /B_/; s/genosp\. //; s/ recomb.*/]/; s/[[:space:]]/_/g; s/_/ /' recA_Bradyrhizobium_vinuesa.fna | grep '>' | tail -5
```

7. Cuando estamos satisfechos con el resultado, guardamos la salida del comando en un archivo usando '>' para redirigir el flujo de STDOUT a un archivo de texto
```{bash}
sed 's/ Bra/ [Bra/; s/|gb.*| /|/; s/Bradyrhizobium /B_/; s/genosp\. //; s/ recomb.*/]/; s/[[:space:]]/_/g; s/_/ /' recA_Bradyrhizobium_vinuesa.fna > recA_Bradyrhizobium_vinuesa.fnaed
```
  - revisemos la salida
```{bash}
grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | head -5 && grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | tail -5
```  

Excelente, ésto ha funcionado perfectamente. Ya pueden decir que dominan las herramientas de filtrado básicas y su combinación en pipelines - ¡¡¡enhorabuena!!!

Pasemos al siguiente nivel.

*****

# Inicios de programación en $Bash$

Vamos a aprender algunos de los elementos esenciales del lenguaje $Bash$, el más poderoso y usado para la <b>programación Shell</b>

## Asignación y uso de **variables**

- La sintaxis básica de asignación es:
```
varName=VALUE
```

- para recuperar el valor de una varible, le añadimos el prefijo <b>\$</b>. Para imprimir el valor asignado a la variable, usamos <b>echo $varName</b>
```{bash}
archivo_de_comandos_linux=linux_commands.tab
echo "$archivo_de_comandos_linux"
```

## Dónde debo guardar mis scripts para que sean visibles desde cualquier directorio del sitema
Como explicábamos en la [tutoral de introducción](https://github.com/vinuesa/intro2linux/tree/master/docs/intro_biocomputo_Linux.pdf), el $Shell$ busca comandos en los directorios especificados en la variable de ambiente **PATH**. Los directorios van separados por $:$ y se leen en ese orden.


```{bash}
echo "PATH = $PATH"
```

Recuerda, es fácile generar una lista de estos directorios más legible con un comando como ésta
```{bash}
echo "$PATH" | sed 's/:/\n/g'
```

### Agregando directorios a la variable de ambiente <b>PATH</b>
A veces necesitamos agregar un nuevo direcotrio a la variable de ambiente **PATH**

- Añade el directorio $HOME/bin al final de la lista de directorios de **PATH**

```
PATH=$PATH:$HOME/bin
```

Nota: en versiones recientes de $Ubuntu$ y otras distribuciones de Linux, el directorio *$HOME/bin* se exporta automáticamente. Revisa tu archivo *$HOME/.bashrc*. Ya que estamos con archivos de configuración, profundicemos un poco más en este tópico.

## Variables de ambiente - **$printenv$**

Al iniciar una <b>sesión local o remota</b>, el $SHELL$ carga en memoria una serie de $VARIABLES\_DE\_AMBIENTE$ que son parte de la configuración del sistema. 

Ya hemos visto algunas antes, pero aquí les muestro algunas muy importantes:

```{bash}
echo "HOME = $HOME"
echo "USER = $USER"
echo "SHELL = $SHELL"
```

Pueden ver todas las variables de su ambiente con el comando $printenv$

<code>printenv | sort | less</code>


### Archivos de configuración **.bash_profile** y **.bashrc**

Si quieres agregar un directorio permanentemente a $PATH$, puedes añadirlo al archivo de configuración $.bash\_profile$ de tu $HOME$ en un servidor remoto o al $.bashrc$ de tu $HOME$ en una máquina local y exportarlo.

```
PATH=$PATH:$HOME/bin

export PATH
```

Cualquier $script$ o programa que pongas ahí con permisos de ejecución (<code>chmod 755 script</code>) será ejecutable desde cualquier directorio del sistema.

### ¿Cuál es el binario o script que estoy usando? - **which**

A veces tenemos un $script$ o un binario (por ejemplo $blastn$) instalado en varios directorios. Esto puede ser intencional o inadvertidamente. El comando $which$ nos indica cual es el $PATH$ del comando que ve el $Shell$, es decir, el primero en $PATH$

```{bash}
which blastn
which perl
which trunc_seq.pl
which transpose_pangenome_matrix.sh
```


## Captura en una varialbe de la salida de un comando con <b>var=$(comando)</b>
```{bash}
wkdir=$(pwd)
dat=$(date | awk '{print $3$2$6}')
h=$(hostname)
echo ">>> working in: $wkdir at <$h> on <$dat>"
```

## Modificación de variables y operaciones con ellas
```{bash}
wkdir=$(pwd)
echo "wkdir: $wkdir"

# 1. cortemos caracteres por la izquierda (todos los caracteres por la izquierda, hasta llegar a último /)
basedir=${wkdir##*/}
echo "basedir: $basedir  # \${wkdir##*/}"

# 2. cortemos caracteres por la derecha (cualqier caracter hasta llegar a /)
echo "path to basedir: ${wkdir%/*}  # \${wkdir%/*}"

# 3. contar el número de caracteres (longitud) de la variable
echo "basedir has ${#basedir} characters  # \${#basedir}"

```

## **Condicionales**
- La sintaxis básica de un condicional simple en formato de una línea es así

<code>if [ condición ]; then orden1; orden2 ...; fi</code>

- también hay una versión más corta para test siples

<code>[ condición ] && setecia1 && sentencia2</code>

- En un script, lo escibimos generalmente como un bloque indentado, para mejor legibilidad

```
if [ condición ]; then 
    orden1
    orden2 
fi
```

### Comparación de íntegros en condicionales
```{bash}
i=5
j=3

if [ "$i" -lt "$j" ]; then
   echo "$i < $j"
elif [ "$i" -gt "$j" ]; then
   echo "$i > $j "
fi
```

### Comparación de cadenas de caracteres en condicionales
```{bash}
c=carla
j=juan

if [ "$c" == "$j" ]; then
   echo "$c = $j"
elif [ "$c" != "$j" ]; then
   echo "c:$c != j:$j "
fi
```

### Comprobación de la existencia de un archivo de tamaño > 0 bytes
```{bash}
touch empty_file
ls -l empty_file
ls -l *gz
f=$(ls *gz)

if [ -e empty_file  ]; then
   echo "empty_file file exists"
fi

if [ ! -s empty_file  ]; then
   echo "empty_file file exists but is empty"
fi

if [ -s "$f" ]; then
   size=$(du -h assembly_summary.txt.gz | cut -f1)
   # o tambien 
   # size=$(ls -hs assembly_summary.txt.gz | cut -d' ' -f1)
   echo "$f exists and has size: $size"
fi
```

### La versión corta de test **[ condición ] && comando1 && comando2** ...
```{bash}
# también podemos usar la versión corta del test:
f=$(ls *.txt.gz)
[ -s "$f" ] && echo "$f exists and is non-empty"
```

### **if; elif; else**
```{bash}
  if [[ "$OSTYPE" == "linux-gnu" ]]
  then
    OS='linux'
    no_cores=$(awk '/^processor/{n+=1}END{print n}' /proc/cpuinfo)
    host=$(hostname)
    echo "running on $host under $OS with $no_cores cores :)"
  elif [[ "$OSTYPE" == "darwin"* ]]
  then
    OS='darwin'
    no_cores=$(sysctl -n hw.ncpu)
    host=$(hostname)
    echo "running on $host under $OS with $no_cores cores :)"
  else
       OS='windows'
       echo "oh no! another windows box :( ... you should better change to GNU/linux :) "
  fi
```


## **Bucles for**
la sintaxis general de un bucle for en $Bash$ es:

<code>for ALIAS in LIST; do CMD1; CMD2; done</code>

donde el usuario tiene que cambiar los términos en mayúsculas por opciones concretas. ALIAS es el nombre de una variable temporal a la que se asigna secuencialmente cada valor de LIST. 

Así por ejemplo, si tuviéramos en un directorio muchos archivos en formato FASTA con secuencias homólogas y extensión *.faa, podríamos alinearlas secuencialmente con $clustalo$, un excelente alineador, con un comando como el siguiente:

### Sintaxis y ejemplo de bucle for en una línea
```
# sintaxis de bucle for en una línea
for file in *.faa; do clustalo -i $file -o ${file%.*}_cluoAln.faa; done
```

### Sintaxis y ejemplo de bucle for en múltiples líneas con indentación
```
# sintaxis de bucle for en múltiples líneas, como se escribiría normalmente en un script
for file in *.faa
do 
    # llamada estándar a clustalo: -i <infile> -o <outfile>
    clustalo -i $file -o ${file%.*}_cluoAln.faa
done
```

Nota 1: Genera un directorio temporal fuera del directorio de la distribución $intro2linux$ y pon en él ligas simbólicas a los archivos *.faa* que hay en el directorio *data/faa_files* de la distribución. En dicho directorio corre el comando indicado arriba.

Nota 2: si quieres aprender más sobre alineamientos múltiples y alineadores, puedes consultar el [tutorial sobre alineamientos múltiples](https://github.com/vinuesa/TIB-filoinfo#sesi%C3%B3n-4-alineamientos-m%C3%BAltiples-teor%C3%ADa-y-pr%C3%A1cticas) que preparé para los [Talleres Internacionales de Bioinformática - 2019 (TIB19)](https://github.com/vinuesa/TIB-filoinfo).


### Generación de archivos FASTA especie-específicos combinando bucle $for$ con herramientas de filtardo
En este ejercicio continuaremos procesando el archivo *recA_Bradyrhizobium_vinuesa.fnaed* que generamos en la sección de [filtrado de cabeceras de archivos FASTA](https://vinuesa.github.io/intro2linux/#edici%C3%B3n-de-las-cabeceras-fasta-mediante-herramientas-de-filtrado-de-UNIX-1).

El reto que les planteo es escribir archivos FASTA especie-específicos en base a *recA_Bradyrhizobium_vinuesa.fnaed*.

Es decir, necesitamos escribir tantos archivos FASTA como especies diferentes tenemos en el archivo fuente.
Para ello nos va a ser muy útil el $script$ [**filter_fasta_sequences.awk**](#generaci%C3%B3n-de-archivos-especie-espec%C3%ADficos) 
que escribimos al final de la sección de $gawk$. 

Vamos por partes:
- recordemos la estructura del archivo fuente
```{bash}
grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | head -5
grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | tail -5
```
- tenemos que generar la lista no redundante de especies en base al patrón de las cabeceras FASTA que desplegamos arriba
```{bash}
for sp in $(grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | cut -d_ -f2 | sort -u | sed 's/\[//')
do 
  echo "processing species $sp ..."
done
```
- ahora es trivial insertar nuestra herramienta de $gawk$, diseñada justo para filtrar secuencias usando nombres de especies 
```{bash}
for sp in $(grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | cut -d_ -f2 | sort -u | sed 's/\[//')
do 
  echo "processing species $sp ..."
  ./filter_fasta_sequences.awk "$sp" recA_Bradyrhizobium_vinuesa.fnaed > "recA_B_${sp}.fasta"
done
```
- como siempre, revisemos la salida, asegurándonos primero que los archivos existen y no están vacíos
```{bash}
ls -ltr *fasta
```
- ahora nos aseguramos que son especie-específicos
```{bash}
for sp in $(grep '^>' recA_Bradyrhizobium_vinuesa.fnaed | cut -d_ -f2 | sort -u | sed 's/\[//')
do 
  echo "processing species $sp ..."
  grep "$sp" "recA_B_${sp}.fasta" | head -2
  echo '-------------------------------------'
  grep "$sp" "recA_B_${sp}.fasta" | tail -2
  echo '.....................................'
done

```

<!--

### Otra solución al problema: convertir archivos FASTA a formato "FASTAB" usando **perl** 1-liners

Vamos a transformar los FASTAS de tal manera que las secuencias queden en la 
misma línea que su cabecera, separada de ésta por un tabulador. Esto puede ser muy útil para 
filtrar el archivo resultante con grep. Veamos un ejemplo:

```{bash}
perl -pe 'unless(/^>/){s/\n//g}; if(/>/){s/\n/\t/g}; s/>/\n>/' recA_Bradyrhizobium_vinuesa.fnaed | head
```

```{bash}
perl -pe 'unless(/^>/){s/\n//g}; if(/>/){s/\n/\t/g}; s/>/\n>/' recA_Bradyrhizobium_vinuesa.fnaed > recA_Bradyrhizobium_vinuesa.fnaedtab
```

- Filtrar  el archivo fnaedtab generado en 9 para obtener solo las secuencias de B._yuanmingense del mismo, guardarlo en un archivo y convertirlo de nuevo a formato FASTA.  

```{bash}
grep yuanmingense recA_Bradyrhizobium_vinuesa.fnaedtab | head -5
```


```{bash}
grep yuanmingense recA_Bradyrhizobium_vinuesa.fnaedtab > recA_Byuanmingense.fnaedtab
```

- Estas dos lineas no contienen nada nuevo en cuanto a sintaxis. Simplemente llamamos a perl para sustituir los tabuladores por saltos de linea y asi reconstituir el FASTA.  
```{bash}
perl -pe 'if(/^>/){s/\t/\n/}' recA_Byuanmingense.fnaedtab | head -5
```

```{bash}
perl -pe 'if(/^>/){s/\t/\n/}' recA_Byuanmingense.fnaedtab > recA_Byuanmingense.fna
```


- Llamar a un bucle for de shell para generar archivos fastab para todas las especies
```{bash}
for sp in $(grep '^>' recA_Bradyrhizobium_vinuesa.fnaedtab | cut -d_ -f2 | sort -u | sed 's/\[//'); do 
   grep "$sp" recA_Bradyrhizobium_vinuesa.fnaedtab > "recA_B_${sp}.fnaedtab"
done
```

- Veamos el resultado
```{bash}
ls *fnaedtab
```

```{bash}
head -5 recA_B_japonicum.fnaedtab
```


- Finalmente convertimos todos los archivos fnatabed a FASTA con el siguiente bucle for
```{bash}
for file in $(ls *fnaedtab | grep -v vinuesa); do perl -pe 'if(/^>/){s/\t/\n/}' $file > ${file%.*}.fas; done
```

- Visualicemos las cabeceras de dos archivos FASTA especie-específicos
```{bash}
grep '>' recA_B_japonicum.fas | head -5
```

- y confirmemos que son fastas regulares
```{bash}
head -6 recA_B_japonicum.fas
```

Esta segunda solución es sin duda más engorrosa. 

-->

## Reto de programación - ejercicio de parseo de archivos FASTA

Como ejercicio, para repasar lo que hemos aprendido en esta sesión les propongo repetir el ejercicio de parseo de archivos FASTA 
pero con secuencias del gen <i>rpoB</i> de  <i>Bradyrhizobium</i>

### Inspección y estadísticas básicas de las secuencias descargadas
1. Descargar las secuencias de NCBI usando el portal ENTREZ nucleotides con: 'Bradyrhizobium[orgn] AND vinuesa[auth] AND rpoB[gene]'
2. Renombra el archivo descargado a rpoB_Bradyrhizobium_vinuesa.fna
3. ¿Cuántas secuencias hay en el archivo rpoB_Bradyrhizobium_vinuesa.fna?
4. Explora la cabecera y cola del archivo con $head$ y $tail$
5. Despliega las 5 primeras lineas de cabeceras fasta usando $grep$ y $head$ para explorar su estructura en detalle
6. Calcula el número de géneros que contiene el archivo FASTA
7. Calcula el número de especies que contiene el archivo FASTA
8. Imprime una lista ordenada de mayor a menor, del numero de especies que contiene el archivo FASTA

### Edición de las cabeceras FASTA mediante herramientas de filtrado de UNIX
1. Explora nuevamente todas las cabeceras FASTA del archivo rpoB_Bradyrhizobium_vinuesa.fna usando $grep$ y $less$
2. Simplifica las cabeceras FASTA usando el comando $sed$ (stream editor)

El objetivo es eliminar redundancia y longitud excesiva de las cabeceras FASTA, así como todos los caracteres 
'( , ; : )' que impedirían el despliegue de un árbol filogenético, al tratarse de caracteres reservados del formato NEWICK.
Dejar solo el numero de accesión, así como el género, especie y cepa indicados entre corchetes.

3. Cuando estén satisfechos con el resultado, guarden la salida del comando en un archivo llamado rpoB_Bradyrhizobium_vinuesa.fnaed

### Generación de archivos especie específicos de secuencias *rpoB*
Al igual que en el ejemplo con secuencias *recA*, genera una lista de nombres de especie únicos y usa un bucle $for$ para pasar los elementos de la misma al $script$ *filter_fasta_sequences.awk* que discutimos en la [sección de $AWK$](#el-scritp-filter_fasta_sequences.awk), para generar los archivos especie-específicos de secuencias. 


<!--

### Ejemplo de bucle for, acoplado a las herramientas de filtrado y de manipulación de variables
La idea del ejercicio es generar archivos a partir de linux_basic_commands.tab que contengan sólo los comandos de cada clase, nombrando a los archivo resultantes con el valor de dicha clase, almacenados en la segunda columna de la tabla

```{bash}
# veamos la cabecera y cola del archivo linux_basic_commands.tab
head linux_basic_commands.tab
echo '--------------------------------------------------'
tail linux_basic_commands.tab
```

Antes de correr el bucle, lista los archivos en el directorio de trabajo
```
# veamos el contenido del directorio antes de correr el bucle
ls
```

Ahora el bucle. En este caso ALIAS=type y LIST corresponde a la lista de valores únicos almacenados en la segunda columna de la tabla: 

<code>$(cut -f2 linux_basic_commands.tab | sort -u)</code>
```{bash}
#>>> Ejemplo integrativo: usa un bucle for, acoplado a las herramientas de filtrado arriba mostradas,
#    para generar archivos que contengan solo los comandos de las diferentes categorias
#    nombrando a los archivos por estas

# for type in $(cut -f2 linux_basic_commands.tab | sort -u); do grep "$type" linux_basic_commands.tab > ${type}.cmds; done
for type in $(cut -f2 linux_basic_commands.tab | cut -d_ -f1 | grep -v Category); do
    grep -w "$type" linux_basic_commands.tab > ${type}.cmds
done

```

Y voilà:
```{bash}
# veamos el contenido del directorio después de correr el bucle
ls *.cmds
```

```{bash}
# veamos el contenido de uno de los nuevos archivos generados
cat programming.cmds

```

-->

```{bash}
# finalmente borremos los nuevos archivos generados en los ejercicios anteriores 
#   para mantener limpio nuestro directorio de trabajo ;)
rm *.cmds
rm *fnaed *.fnaedtab *.fas empty_file Pseudomonas_species* *.fasta
```

*****

# **$Bash$ scripting** - siguientes pasos

En esta sección veremos ejemplos muy sencillos de **Shell scripts** escritos en $Bash$ que integran varios aspectos de la sintaxis básica del lenguaje descritas en la sección anterior, así como extensiones adicionales como son **argumentos posicionales** y **funciones**

## la "**shebang line**" (#!/usr/bin/env bash) y permisos de ejecución - el script *ls_dir*
Se conocen como *scripts* a archivos de texto plano (codificación ASCII) que contienen los comandos a ser ejecutados secuencialmente por un **intérprete de comandos** particular,
como $bash$, $perl$, $R$ ... 

Les muestro abajo el código del script *ls_dir*, que pueden descargar desde el [repositorio githut de intro2linux](https://github.com/vinuesa/intro2linux/blob/master/ls_dir)

```
#!/usr/bin/env bash

#  the 1st line in a script is the so-called shebang line
#  which indicates the system which command interpreter 
#  should read and execute the following code, bash in this case
#  The shebang line shown above, is a portable version for bash scripts

# AUTHOR: Pablo Vinuesa
# AIM: learning basic BASH-programming constructs for intro2linux
# https://github.com/vinuesa/intro2linux

for file in $(ls)
do 
    if [ -d "$file" ]
    then 
        echo "$file"
    fi
done
```

Puedes copiar el código a un archivo que vas a nombrar como ls_dir. Usa para ello el comando $cat$, de la manera abajo indicada:

```
cat > ls_dir
PEGA AQUÍ EL CÓDIGO ARRIBA MOSTRADO
CTRL-D
```

El script *ls_dir* busca los directorios presentes en el directorio actual usando un bucle for para analizar cada archivo econtrado por $ls$, 
evaluando seguidamente si el archivo en turno es un directorio con un condicional <code>if [ -d "$file" ]</code>

Noten que la primera línea inicia con lo que se conoce como una **shebang line**:

<code>#!/usr/bin/env bash</code>

Esta línea, en la cabecera del archivo (¡sin dejar especios a la izquierda o arriba de la línea!), 
le indica al sistema operativo qué **intérprete de comandos** usar para la ejecución del código que sigue. 
En este caso se llama al intérprete de comandos $bash$

Noten también que cualquier línea que inicie con **#** después de la shebang line, representa un comentario,
es decir, que el texto que sigue al gato no es interpretado por $bash$

Para ejecutar el script como si fuera un comando cualquiera de $Linux$, le damos **permisos de ejecución**:

```
chmod 755 ls_dir
```

Comprueba los permisos:

<code>$ ls -l ls_dir</code>
```
-rwxr-xr-x 1 vinuesa vinuesa 493 ago 11 18:37 ls_dir
```
Como puedes ver el usuario, el grupo al que pertenece y todos los demás pueden ejecutarlo ($x$)

Finalmente copia o mueve el script a un directorio que esté en el $PATH$, típicamente a $HOME/bin$
```
mv ls_dir ~/bin
```

y ya puedes usarlo:
```{bash}
./ls_dir
```

Nota: el script *ls_dir* funciona perfectamente, pero no es la manera más eficiente de buscar directorios. La función $find$ de $Linux$ es mucho más eficiente.

## **Funciones** y **argumentos posicionales** en $Bash$ - el script *find_dir*

Los scripts pueden recibir opciones dadas por los usuarios para comportarse acorde a ellas. Esto les da gran versatilidad. 

La manera más sencialla de pasarle opciones al script es mediante **argumentos posicionales**, como los usados por *find_dir*, que pueden descargar desde el [repositorio githut de intro2linux](https://github.com/vinuesa/intro2linux/blob/master/find_dir)

```
#!/usr/bin/env bash

#  The 1st line in a script is the so-called shebang line
#  which indicates the system which command interpreter 
#  should read and execute the following code, bash in this case
#  The shebang line shown above, is a portable version for bash scripts

# AUTHOR: Pablo Vinuesa
# AIM: learning basic BASH-programming constructs for intro2linux
# https://github.com/vinuesa/intro2linux

# global variables
progname=${0##*/}      # find_dir
VERSION='0.1_29Jul19' 

# Function definition
function print_help()
{
   # this is a so-called HERE-DOC, to easily print out formatted text
   cat << HELP

    $progname v$VERSION usage synopsis:

    $progname <int [maximal search depth for (sub)directories, e.g.:1>

    AIM: find directories below the current one with the desired max_depth 

    USAGE EXAMPLES:
          $progname 1
          $progname 2

HELP

exit 1
}

# capture user-provided arguments in variables $1, $2 ... $9
# and save them to named variables, for better readability
max_depth=$1
type=${2:-d}  # if not provided, the second argument is set to 'd' by default.

# check arguments
[ -z $max_depth ] && print_help

# execute find command with the provided arguments
find . -maxdepth $max_depth -type $type
```

Guarda el código arriba mostrado en un archivo llamado *find_dir*, dale permisos de ejecución, y cópialo a $HOME/bin$, 
como se mostró en el ejemplo anterior.

*find_dir* hace la siguiente llamada a $find$

<code>find . -maxdepth $max_depth -type $type</code>

para encontrar directorios y subdirectorios por debajo del acutal, al nivel de profundidad indicado por el usuario,
quien pasa un íntegro al script como único argumento

*find_dir* introduce la función *print_help()*, 
```
function print_help(){
  YOUR CODE GOES INHERE ...
}
```
que simplemente imprime un mensaje de ayuda si es llamado sin argumentos

<code>[ -z $max_depth ] && print_help</code>

Probemos el script:

- primero llamando al script sin argumento:

<code>find_dir</code>
```
    find_dir v0.1_29Jul19 usage synopsis:

    find_dir <int [maximal search depth for (sub)directories, e.g.:1>

    AIM: find directories below the current one at the desired max_depth 

    USAGE EXAMPLES
          find_dir 1
          find_dir 2

```

- ahora pasándole un argumento:
```{bash}
  ./find_dir 1
```


## Scripts que reciben argumentos|parámetros posicionales con interfaz de usario simple

En el [repositorio githut de intro2linux](https://github.com/vinuesa/intro2linux/) encontrarán los scripts *align_seqs_with_clustal_or_muscle.sh* y *convert_alnFormats_using_clustalw.sh* para alineamiento múltiple de secuencias e interconversión de formatos. Son ejemplos de $scripts$ que reciben parámetros posicionales y que despliegan un mensaje de ayuda, indicando los argumentos posicionales requeridos.

```
./align_seqs_with_clustal_or_muscle.sh
```

que imprime lo siguiente a **STDOUT**

```
# ./align_seqs_with_clustal_or_muscle.sh vers.1.2 needs two arguments: 
#	1) the input fasta file extension_name <[fas|fasta|fna|faa]>
#	2) the alignment program to use <[muscle|clustalw]>
# usage example: ./align_seqs_with_clustal_or_muscle.sh fna muscle

# NOTE: the script assumes that clustalw and muscle are in found in $PATH
# will check now for the presence of both binaries in $PATH
# looks ok, found clustalw here: /usr/bin/clustalw
# looks ok, found muscle here: /usr/bin/muscle  
```


- **Ejercicio**:  
  1. Explora también el código del script [convert_alnFormats_using_clustalw.sh](https://github.com/vinuesa/intro2linux/blob/master/convert_alnFormats_using_clustalw.sh),
que permite hacer inteconversión de formatos con clustalw.
  2. Explora también el código del script [align_seqs_with_clustal_or_muscle.sh](https://github.com/vinuesa/intro2linux/blob/master/align_seqs_with_clustal_or_muscle.sh), que permite hacer  alineamientos múltiples con $clustalw$ o con $muscle$, dos programas muy populares para este fin.

Los alineamientos múltiples y las llamadas a $clustalw$ se explican en [sesión4_alineamientos](https://github.com/vinuesa/intro2linux/tree/master/docs/sesion4_alineamientos).

Con lo aprendido hasta ahora y los comentarios que documentan los scripts, deberías entender lo que hacen.

Vuelve a explorar ambos scripts después de haber revisado la [sesión4_alineamientos](https://github.com/vinuesa/intro2linux/tree/master/docs/sesion4_alineamientos).


## Scripts con interfaz de usuario, paso de opciones y su parseo con **getopts**

Para scripts más complejos, que pueden recibir muchas opciones, es mejor escribir una interfaz de usuario que permita el parseo de opciones pasadas al script desde la línea de comandos y describir adecuadamente estas opciones en un menú de ayuda que se despliega en pantalla.

Les presento seguidamente secciones del archivo de templado o machote para escribir scripts con parseo de opciones llamado *bash_script_template_with_getopts.sh* disponible en el [repositorio githut de intro2linux](https://github.com/vinuesa/intro2linux/blob/master/bash_script_template_with_getopts.sh)

```
function print_help()
{
   cat <<EOF
   $progname v.$VERSION usage:
   
   REQUIRED:
    -a <string> alignment algorithm [clustalo|muscle; default:$alignment_algorithm]
    -i <string> input fasta file name
    -h <FLAG> print this help
    -v <FLAG> print version
    -R <integer> RUNMODE
          1     standard msa
	        2     profile-profile alingnment
	        3     sequence to profile alignment
    
   OPTIONAL:
    
  
   NOTE1: XXX
   
   TODO: 

EOF

   check_dependencies
   
   exit 2  
}
#----------------------------------------------------------------------------------------- 


#------------------------------------#
#----------- GET OPTIONS ------------#
#------------------------------------#

input_fasta=
runmode=

alignment_algorithm=clustalo
DEBUG=0

# See bash cookbook 13.1 and 13.2
while getopts ':i:d:R:hD?:' OPTIONS
do
   case $OPTIONS in

   a)   alignment_algorithm=$OPTARG
        ;;
   i)   input_fasta=$OPTARG
        ;;
   h)   print_help
        ;;
   v)   echo "$progname v.$VERSION"
        ;;
   R)   runmode=$OPTARG
        ;;
   D)   DEBUG=1
        ;;
   \:)   printf "argument missing from -%s option\n" $OPTARG
   	 print_help
     	 exit 2 
     	 ;;
   \?)   echo "need the following args: "
   	 print_help
         exit 3
	 ;;
    *)   echo "An  unexpected parsing error occurred"
         echo
         print_help
	 exit 4
	 ;;	 
   esac >&2   # print the ERROR MESSAGES to STDERR
done

shift $((OPTIND - 1))

if [ -z "$input_fasta_extension" ]
then
       echo "# ERROR: no input fasta file extension defined!"
       print_help
       exit 1    
fi

if [ -z "$runmode" ]
then
       echo "# ERROR: no runmode defined!"
       print_help
       exit 1    
fi

if [ -z "$DEBUG" ]
then
     DEBUG=0 
fi

```

Ahora puedo llamar a este script (templado) de la siguiente manera

```
bash bash_script_template_with_getopts.sh
```

que imprime lo siguiente a **STDOUT**

```
# ERROR: no input fasta file extension defined!
   bash_script_template_with_getopts.sh v.0.1 usage:
   
   REQUIRED:
    -a <string> alignment algorithm [clustalo|muscle; default:clustalo]
    -i <string> input fasta file name
    -h <FLAG> print this help
    -v <FLAG> print version
    -R <integer> RUNMODE
          1     standard msa
	        2     profile-profile alingnment
	        3     sequence to profile alignment
    
   OPTIONAL:
    
  
   NOTE1: XXX
   
   TODO: 


# Run check_dependencies() ... looks good: all required binaries and perl scripts are in place.

```

Haz una copia del esqueleto que provee *bash_script_template_with_getopts.sh* para modificarlo y adaptarlo a lo que necesite tu $script$. El templado te facilita el arranque.



- **Ejercicio**
 1. usa el archivo de machote *bash_script_template_with_getopts.sh* para modificar el script  [align_seqs_with_clustal_or_muscle.sh](https://github.com/vinuesa/intro2linux/blob/master/docs/bin/align_seqs_with_clustal_or_muscle.sh), añadiéndole una interfaz de usuario.

***** 

# Consideraciones finales y referencias recomendadas para continuar aprendiendo Bash
Llegamos al final de este tutorial, enhorabuena!!! Has aprendido mucho, pero queda todavía queda mucho camino por andar. Bash sin duda es muy útil si vas a usarlo como *lenguaje pegamento* para conectar múltiples utilerías o programas, pero no es el apropiado para escribir programas complejos que tienen que hacer procesamiento numérico, gráfico, estadístico de datos, interactuar con bases de datos, parsear archivos de estructura compleja, etc. Para ello deberás aprender otros lenguajes más poderosos, capaces de manejar estructuras de datos complejas y librerías o paquetes especializados de código para ello. Pero aprender primero Shell es sin duda lo mejor que puedes hacer para iniciarte en la programación en sistemas UNIX o GNU/Linux. Te facilitará el camino posterior y te permitirá dominar el sistema operativo ya que el $Shell$ es su interfaz programática.

Recomiendo las siguientes guías para apoyarte en tu proceso de aprendizaje del $Shell$. Disfruta el camino, saludos!

- [The GNU Awk User’s Guide](https://www.gnu.org/software/gawk/manual/gawk.html)
- [The GNU Bash Reference Manual](https://www.gnu.org/software/bash/manual/bash.html)
- [The GNU/Linux Documentation Project - LDP](https://tldp.org/)
- [Advanced Bash-Scripting Guide - LDP, Mendel Cooper](https://tldp.org/LDP/abs/abs-guide.pdf)
- [Advanced Bash Scripting, Michael F. Herbst, Uni Heidelberg](https://michael-herbst.com/teaching/advanced-bash-scripting-2017/advanced-bash-scripting-2017/notes.pdf)

